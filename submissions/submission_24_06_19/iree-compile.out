// -----// IR Dump After AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
    return %0 : tensor<3x2xf32>
  }
}


// -----// IR Dump After TosaToMLProgram (tosa-to-mlprogram) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
    return %0 : tensor<3x2xf32>
  }
}


// -----// IR Dump After TosaToSCF (tosa-to-scf) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
    return %0 : tensor<3x2xf32>
  }
}


// -----// IR Dump After TosaMakeBroadcastable (tosa-make-broadcastable) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaToArith (tosa-to-arith) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaToTensor (tosa-to-tensor) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaToLinalgExt (iree-tosa-to-linalg-ext) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaOptionalDecompositions (tosa-optional-decompositions) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaInferShapes (tosa-infer-shapes) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaMakeBroadcastable (tosa-make-broadcastable) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaToLinalgNamed (tosa-to-linalg-named) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaLayerwiseConstantFoldPass (tosa-layerwise-constant-fold) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaMakeBroadcastable (tosa-make-broadcastable) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  return %0 : tensor<3x2xf32>
}

// -----// IR Dump After TosaValidation (tosa-validate) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tosa.add %arg0, %arg1 : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
    return %0 : tensor<3x2xf32>
  }
}


// -----// IR Dump After TosaToLinalg (tosa-to-linalg) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  return %1 : tensor<3x2xf32>
}

// -----// IR Dump After Converti48Toi64 (iree-tosa-convert-i48-to-i64) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  return %1 : tensor<3x2xf32>
}

// -----// IR Dump After TosaToArith (tosa-to-arith) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  return %1 : tensor<3x2xf32>
}

// -----// IR Dump After TosaToTensor (tosa-to-tensor) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  return %1 : tensor<3x2xf32>
}

// -----// IR Dump After StripSignedness (iree-tosa-strip-signedness) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  return %1 : tensor<3x2xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  return %1 : tensor<3x2xf32>
}

// -----// IR Dump After LinalgQuantizedMatmulToMatmulPass (iree-linalg-quantized-matmul-to-matmul) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  return %1 : tensor<3x2xf32>
}

// -----// IR Dump After LinalgQuantizedConvToConvPass (iree-linalg-quantized-conv-to-conv) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  return %1 : tensor<3x2xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  return %1 : tensor<3x2xf32>
}

// -----// IR Dump After VerifyCompilerTOSAInputLegality (iree-tosa-verify-compiler-input-legality) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tensor.empty() : tensor<3x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.addf %in, %in_0 : f32
      linalg.yield %2 : f32
    } -> tensor<3x2xf32>
    return %1 : tensor<3x2xf32>
  }
}


// -----// IR Dump After AutoInputConversionPipeline (iree-auto-input-conversion) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  func.func @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tensor.empty() : tensor<3x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.addf %in, %in_0 : f32
      linalg.yield %2 : f32
    } -> tensor<3x2xf32>
    return %1 : tensor<3x2xf32>
  }
}


// -----// IR Dump After IREEImportPublic (iree-import-public) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tensor.empty() : tensor<3x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.addf %in, %in_0 : f32
      linalg.yield %2 : f32
    } -> tensor<3x2xf32>
    util.return %1 : tensor<3x2xf32>
  }
}


// -----// IR Dump After ImportMLProgram (iree-import-ml-program) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tensor.empty() : tensor<3x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.addf %in, %in_0 : f32
      linalg.yield %2 : f32
    } -> tensor<3x2xf32>
    util.return %1 : tensor<3x2xf32>
  }
}


// -----// IR Dump After SanitizeModuleNames (iree-sanitize-module-names) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tensor.empty() : tensor<3x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.addf %in, %in_0 : f32
      linalg.yield %2 : f32
    } -> tensor<3x2xf32>
    util.return %1 : tensor<3x2xf32>
  }
}


// -----// IR Dump After ConvertMeshToFlowPass (iree-convert-mesh-to-flow) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tensor.empty() : tensor<3x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.addf %in, %in_0 : f32
      linalg.yield %2 : f32
    } -> tensor<3x2xf32>
    util.return %1 : tensor<3x2xf32>
  }
}


// -----// IR Dump After DemoteF64ToF32 (iree-input-conversion-demote-f64-to-f32) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tensor.empty() : tensor<3x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.addf %in, %in_0 : f32
      linalg.yield %2 : f32
    } -> tensor<3x2xf32>
    util.return %1 : tensor<3x2xf32>
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::ConvertStreamableOpsPass (iree-abi-convert-streamable-ops) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tensor.empty() : tensor<3x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.addf %in, %in_0 : f32
      linalg.yield %2 : f32
    } -> tensor<3x2xf32>
    util.return %1 : tensor<3x2xf32>
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::WrapEntryPointsPass (iree-abi-wrap-entry-points) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = util.call @_add(%0, %1) : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
  util.func private @_add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
    %0 = tensor.empty() : tensor<3x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %2 = arith.addf %in, %in_0 : f32
      linalg.yield %2 : f32
    } -> tensor<3x2xf32>
    util.return %1 : tensor<3x2xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @_add(%arg0: tensor<3x2xf32>, %arg1: tensor<1x1xf32>) -> tensor<3x2xf32> {
  %0 = tensor.empty() : tensor<3x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%0 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %2 = arith.addf %in, %in_0 : f32
    linalg.yield %2 : f32
  } -> tensor<3x2xf32>
  util.return %1 : tensor<3x2xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = util.call @_add(%0, %1) : (tensor<3x2xf32>, tensor<1x1xf32>) -> tensor<3x2xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After RemoveZeroExtentTensors (iree-global-opt-remove-zero-extent-tensors) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After DetachElementwiseFromNamedOps (iree-global-opt-detach-elementwise-from-named-ops) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After LinalgNamedOpConversionPass (linalg-named-op-conversion) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Convert1X1FilterConv2DToMatmul (iree-global-opt-convert-1x1-filter-conv2d-to-matmul) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After EraseUnusedLinalgOperands (iree-global-opt-erase-unused-linalg-operands) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ExpandTensorShapes (iree-global-opt-expand-tensor-shapes) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (0, 0)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertElementwiseToLinalgPass (convert-elementwise-to-linalg) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After RaiseSpecialOps (iree-global-opt-raise-special-ops) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After DecomposeConcat (iree-global-opt-decompose-concat) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After GeneralizeLinalgNamedOps (iree-global-opt-generalize-linalg-named-ops) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (0, 0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<3x2xf32>, tensor<1x1xf32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldUnitExtentDimsPass (iree-flow-fold-unit-extent-dims) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After DemoteContractionInputsToBF16 (iree-global-opt-demote-contraction-inputs-to-bf16) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FuseDequantizationMatmul (iree-global-opt-fuse-dequantization-matmul) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SetEncoding (iree-global-opt-set-encoding) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CPUMaterializeUpperBoundTileSize (iree-codegen-cpu-materialize-upper-bound-tile-size) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CPUMaterializeEncoding (iree-codegen-cpu-materialize-encoding) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After MaterializeHomogeneousEncodings (iree-global-opt-materialize-homogeneous-encodings) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyPackUnpack (iree-global-opt-simplify-pack-unpack) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After DataLayoutPropagation (iree-global-opt-data-layout-propagation) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After GeneralizeLinalgNamedOps (iree-global-opt-generalize-linalg-named-ops) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After GlobalLoopInvariantCodeMotion (iree-global-opt-loop-invariant-code-motion) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After HoistIntoGlobals (iree-util-hoist-into-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After JitGlobals (iree-consteval-jit-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After RaiseSpecialOps (iree-global-opt-raise-special-ops) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After VerifyInputLegalityPass (iree-verify-input-legality) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After InjectTensorTracingPass (iree-flow-inject-tensor-tracing) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After TensorPadToTensorInsertSlicePass (iree-flow-tensor-pad-to-tensor-insert-slice) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = tensor.empty() : tensor<3x2xf32>
    %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %5 = arith.addf %in, %in_0 : f32
      linalg.yield %5 : f32
    } -> tensor<3x2xf32>
    %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FusionPreprocessingPass (iree-flow-fusion-preprocessing) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ElementwiseOpFusionPass (iree-flow-elementwise-op-fusion) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After BubbleUpExpandShapesPass (iree-flow-bubble-up-expand-shapes) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ElementwiseOpFusionPass (iree-flow-elementwise-op-fusion) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SinkReshapesPass (iree-flow-sink-reshapes) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FusionOfTensorOpsPass (iree-flow-fusion-of-tensor-ops) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SplitReductionPass (iree-flow-split-reduction-ops) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FusionPreprocessingPass (iree-flow-fusion-preprocessing) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After InterchangeTransposeGenericOpsPass (iree-flow-interchange-transpose-generic-ops) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FormScalarDispatchesPass (iree-flow-form-scalar-dispatches) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %5 = arith.addf %in, %in_0 : f32
    linalg.yield %5 : f32
  } -> tensor<3x2xf32>
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchRegionsPass (iree-flow-form-dispatch-regions) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = flow.dispatch.region -> (tensor<3x2xf32>) {
    %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%2 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %6 = arith.addf %in, %in_0 : f32
      linalg.yield %6 : f32
    } -> tensor<3x2xf32>
    flow.return %5 : tensor<3x2xf32>
  }
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CloneProducersIntoDispatchRegionsPass (iree-flow-clone-producers-into-dispatch-regions) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %3 = flow.dispatch.region -> (tensor<3x2xf32>) {
    %5 = tensor.empty() : tensor<3x2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> ()>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %collapsed : tensor<3x2xf32>, tensor<f32>) outs(%5 : tensor<3x2xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<3x2xf32>
    flow.return %6 : tensor<3x2xf32>
  }
  %4 = hal.tensor.export %3 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CollapseDimensionsPass (iree-flow-collapse-dimensions) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = tensor.empty() : tensor<3x2xf32>
  %collapsed = tensor.collapse_shape %1 [] : tensor<1x1xf32> into tensor<f32>
  %collapsed_0 = tensor.collapse_shape %0 [[0, 1]] : tensor<3x2xf32> into tensor<6xf32>
  %3 = flow.dispatch.region -> (tensor<6xf32>) {
    %5 = tensor.empty() : tensor<6xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed_0, %collapsed : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
    ^bb0(%in: f32, %in_2: f32, %out: f32):
      %7 = arith.addf %in, %in_2 : f32
      linalg.yield %7 : f32
    } -> tensor<6xf32>
    %expanded_1 = tensor.expand_shape %6 [[0, 1]] output_shape [3, 2] : tensor<6xf32> into tensor<3x2xf32>
    flow.return %6 : tensor<6xf32>
  }
  %expanded = tensor.expand_shape %3 [[0, 1]] output_shape [3, 2] : tensor<6xf32> into tensor<3x2xf32>
  %4 = hal.tensor.export %expanded "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchWorkgroupsPass (iree-flow-form-dispatch-workgroups) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch.workgroups(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32> =
      (%arg2: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
    %9 = tensor.empty() : tensor<6xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8 : tensor<6xf32>, tensor<f32>) outs(%9 : tensor<6xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %11 = arith.addf %in, %in_0 : f32
      linalg.yield %11 : f32
    } -> tensor<6xf32>
    flow.dispatch.tensor.store %10, %arg4, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After CaptureDynamicDimsPass (iree-flow-capture-dynamic-dims) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch.workgroups(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32> =
      (%arg2: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
    %9 = tensor.empty() : tensor<6xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8 : tensor<6xf32>, tensor<f32>) outs(%9 : tensor<6xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %11 = arith.addf %in, %in_0 : f32
      linalg.yield %11 : f32
    } -> tensor<6xf32>
    flow.dispatch.tensor.store %10, %arg4, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch.workgroups(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32> =
      (%arg2: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
    %9 = tensor.empty() : tensor<6xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8 : tensor<6xf32>, tensor<f32>) outs(%9 : tensor<6xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %11 = arith.addf %in, %in_0 : f32
      linalg.yield %11 : f32
    } -> tensor<6xf32>
    flow.dispatch.tensor.store %10, %arg4, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch.workgroups(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32> =
      (%arg2: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
    %9 = tensor.empty() : tensor<6xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8 : tensor<6xf32>, tensor<f32>) outs(%9 : tensor<6xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %11 = arith.addf %in, %in_0 : f32
      linalg.yield %11 : f32
    } -> tensor<6xf32>
    flow.dispatch.tensor.store %10, %arg4, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After InitializeEmptyTensorsPass (iree-flow-initialize-empty-tensors) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch.workgroups(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32> =
      (%arg2: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
    %7 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
    %8 = flow.dispatch.tensor.load %arg3, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
    %9 = tensor.empty() : tensor<6xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8 : tensor<6xf32>, tensor<f32>) outs(%9 : tensor<6xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %11 = arith.addf %in, %in_0 : f32
      linalg.yield %11 : f32
    } -> tensor<6xf32>
    flow.dispatch.tensor.store %10, %arg4, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After OutlineDispatchExternsPass (iree-flow-outline-dispatch-externs) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch.workgroups(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32> =
        (%arg2: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg3: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg4: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
      %7 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
      %8 = flow.dispatch.tensor.load %arg3, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
      %9 = tensor.empty() : tensor<6xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%7, %8 : tensor<6xf32>, tensor<f32>) outs(%9 : tensor<6xf32>) {
      ^bb0(%in: f32, %in_0: f32, %out: f32):
        %11 = arith.addf %in, %in_0 : f32
        linalg.yield %11 : f32
      } -> tensor<6xf32>
      flow.dispatch.tensor.store %10, %arg4, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineDispatchRegionsPass (iree-flow-outline-dispatch-regions) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchesPass (iree-flow-annotate-dispatches) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After StripDebugOps (iree-util-strip-debug-ops) //----- //
flow.executable private @add_dispatch_0 {
  flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
      %2 = tensor.empty() : tensor<6xf32>
      %3 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
      ^bb0(%in: f32, %in_0: f32, %out: f32):
        %4 = arith.addf %in, %in_0 : f32
        linalg.yield %4 : f32
      } -> tensor<6xf32>
      flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After DeduplicateExecutablesPass (iree-flow-deduplicate-executables) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After InjectTensorTracingPass (iree-flow-inject-tensor-tracing) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After CleanupTensorShapesPass (iree-flow-cleanup-tensor-shapes) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After OutlineConstantsPass (iree-flow-outline-constants) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInputPass (iree-stream-verify-input) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
  %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
  %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
  %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
  %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
  %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  flow.executable private @add_dispatch_0 {
    flow.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !flow.dispatch.tensor<readonly:tensor<6xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<f32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<6xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %2 = tensor.empty() : tensor<6xf32>
        %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%0, %1 : tensor<6xf32>, tensor<f32>) outs(%2 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %4 = arith.addf %in, %in_0 : f32
          linalg.yield %4 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %3, %arg2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<3x2xf32>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<1x1xf32>
    %2 = flow.tensor.reshape %1 : tensor<1x1xf32> -> tensor<f32>
    %3 = flow.tensor.reshape %0 : tensor<3x2xf32> -> tensor<6xf32>
    %4 = flow.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%3, %2) : (tensor<6xf32>, tensor<f32>) -> tensor<6xf32>
    %5 = flow.tensor.reshape %4 : tensor<6xf32> -> tensor<3x2xf32>
    %6 = hal.tensor.export %5 "output0" : tensor<3x2xf32> -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertToStreamPass (iree-stream-conversion) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof tensor<3x2xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %element_type_f32_0 = hal.element_type<f32> : i32
    %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
    %c1 = arith.constant 1 : index
    %c1_2 = arith.constant 1 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1_2]) type(%element_type_f32_0) encoding(%dense_row_major_1)
    %3 = stream.tensor.sizeof tensor<1x1xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<f32> : index
    %7 = stream.tensor.clone %5 : tensor<1x1xf32> in !stream.resource<*>{%3} -> tensor<f32> in !stream.resource<*>{%6}
    %8 = stream.tensor.sizeof tensor<6xf32> : index
    %9 = stream.tensor.clone %2 : tensor<3x2xf32> in !stream.resource<*>{%0} -> tensor<6xf32> in !stream.resource<*>{%8}
    %c0 = arith.constant 0 : index
    %10 = stream.tensor.sizeof tensor<6xf32> : index
    %11 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%9[%c0 to %8 for %8], %7[%c0 to %6 for %6]) : (!stream.resource<*>{%8}, !stream.resource<*>{%6}) -> !stream.resource<*>{%10}
    %12 = stream.tensor.sizeof tensor<3x2xf32> : index
    %13 = stream.tensor.clone %11 : tensor<6xf32> in !stream.resource<*>{%10} -> tensor<3x2xf32> in !stream.resource<*>{%12}
    %14 = stream.async.transfer %13 : !stream.resource<*>{%12} -> !stream.resource<external>{%12}
    %15 = stream.tensor.export %14 : tensor<3x2xf32> in !stream.resource<external>{%12} -> !hal.buffer_view
    util.return %15 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToTensorsPass (iree-stream-verify-lowering-to-tensors) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof tensor<3x2xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %element_type_f32_0 = hal.element_type<f32> : i32
    %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
    %c1 = arith.constant 1 : index
    %c1_2 = arith.constant 1 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1_2]) type(%element_type_f32_0) encoding(%dense_row_major_1)
    %3 = stream.tensor.sizeof tensor<1x1xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<f32> : index
    %7 = stream.tensor.clone %5 : tensor<1x1xf32> in !stream.resource<*>{%3} -> tensor<f32> in !stream.resource<*>{%6}
    %8 = stream.tensor.sizeof tensor<6xf32> : index
    %9 = stream.tensor.clone %2 : tensor<3x2xf32> in !stream.resource<*>{%0} -> tensor<6xf32> in !stream.resource<*>{%8}
    %c0 = arith.constant 0 : index
    %10 = stream.tensor.sizeof tensor<6xf32> : index
    %11 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%9[%c0 to %8 for %8], %7[%c0 to %6 for %6]) : (!stream.resource<*>{%8}, !stream.resource<*>{%6}) -> !stream.resource<*>{%10}
    %12 = stream.tensor.sizeof tensor<3x2xf32> : index
    %13 = stream.tensor.clone %11 : tensor<6xf32> in !stream.resource<*>{%10} -> tensor<3x2xf32> in !stream.resource<*>{%12}
    %14 = stream.async.transfer %13 : !stream.resource<*>{%12} -> !stream.resource<external>{%12}
    %15 = stream.tensor.export %14 : tensor<3x2xf32> in !stream.resource<external>{%12} -> !hal.buffer_view
    util.return %15 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof tensor<3x2xf32> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %element_type_f32_0 = hal.element_type<f32> : i32
  %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32_0) encoding(%dense_row_major_1)
  %3 = stream.tensor.sizeof tensor<1x1xf32> : index
  %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
  %6 = stream.tensor.sizeof tensor<f32> : index
  %7 = stream.tensor.sizeof tensor<6xf32> : index
  %8 = stream.tensor.sizeof tensor<6xf32> : index
  %9 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%2[%c0 to %7 for %7], %5[%c0 to %6 for %6]) : (!stream.resource<*>{%7}, !stream.resource<*>{%6}) -> !stream.resource<*>{%8}
  %10 = stream.tensor.sizeof tensor<3x2xf32> : index
  %11 = stream.async.transfer %9 : !stream.resource<*>{%10} -> !stream.resource<external>{%10}
  %12 = stream.tensor.export %11 : tensor<3x2xf32> in !stream.resource<external>{%10} -> !hal.buffer_view
  util.return %12 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof tensor<3x2xf32> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %3 = stream.tensor.sizeof tensor<1x1xf32> : index
  %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
  %6 = stream.tensor.sizeof tensor<f32> : index
  %7 = stream.tensor.sizeof tensor<6xf32> : index
  %8 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%2[%c0 to %7 for %7], %5[%c0 to %6 for %6]) : (!stream.resource<*>{%7}, !stream.resource<*>{%6}) -> !stream.resource<*>{%7}
  %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %10 = stream.tensor.export %9 : tensor<3x2xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
  util.return %10 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof tensor<3x2xf32> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %3 = stream.tensor.sizeof tensor<1x1xf32> : index
  %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
  %6 = stream.tensor.sizeof tensor<f32> : index
  %7 = stream.tensor.sizeof tensor<6xf32> : index
  %8 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%2[%c0 to %7 for %7], %5[%c0 to %6 for %6]) : (!stream.resource<*>{%7}, !stream.resource<*>{%6}) -> !stream.resource<*>{%7}
  %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %10 = stream.tensor.export %9 : tensor<3x2xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
  util.return %10 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof tensor<3x2xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %3 = stream.tensor.sizeof tensor<1x1xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<f32> : index
    %7 = stream.tensor.sizeof tensor<6xf32> : index
    %8 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%2[%c0 to %7 for %7], %5[%c0 to %6 for %6]) : (!stream.resource<*>{%7}, !stream.resource<*>{%6}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<3x2xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    util.return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof tensor<3x2xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %3 = stream.tensor.sizeof tensor<1x1xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<f32> : index
    %7 = stream.tensor.sizeof tensor<6xf32> : index
    %8 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%2[%c0 to %7 for %7], %5[%c0 to %6 for %6]) : (!stream.resource<*>{%7}, !stream.resource<*>{%6}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<3x2xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    util.return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof tensor<3x2xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %3 = stream.tensor.sizeof tensor<1x1xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<f32> : index
    %7 = stream.tensor.sizeof tensor<6xf32> : index
    %8 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%2[%c0 to %7 for %7], %5[%c0 to %6 for %6]) : (!stream.resource<*>{%7}, !stream.resource<*>{%6}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<3x2xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    util.return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof tensor<3x2xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %3 = stream.tensor.sizeof tensor<1x1xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<f32> : index
    %7 = stream.tensor.sizeof tensor<6xf32> : index
    %8 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%2[%c0 to %7 for %7], %5[%c0 to %6 for %6]) : (!stream.resource<*>{%7}, !stream.resource<*>{%6}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<3x2xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    util.return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof tensor<3x2xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %3 = stream.tensor.sizeof tensor<1x1xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<f32> : index
    %7 = stream.tensor.sizeof tensor<6xf32> : index
    %8 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%2[%c0 to %7 for %7], %5[%c0 to %6 for %6]) : (!stream.resource<*>{%7}, !stream.resource<*>{%6}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<3x2xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    util.return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After EncodeDeviceTensorsPass (iree-stream-encode-device-tensors) //----- //
stream.executable private @add_dispatch_0 {
  stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
      %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
      %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
      %5 = tensor.empty() : tensor<6xf32>
      %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
      ^bb0(%in: f32, %in_0: f32, %out: f32):
        %7 = arith.addf %in, %in_0 : f32
        linalg.yield %7 : f32
      } -> tensor<6xf32>
      flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
      return
    }
  }
}

// -----// IR Dump After EncodeHostTensorsPass (iree-stream-encode-host-tensors) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
  %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
  %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
  %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
  %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
  %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
  %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
  %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
  %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
    %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
    %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
    %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
    %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsyncResourcesPass (iree-stream-verify-lowering-to-async-resources) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
    %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeCopyOnWritePass (iree-stream-materialize-copy-on-write) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
  %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
  %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
  %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
  %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After ElideAsyncCopiesPass (iree-stream-elide-async-copies) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
    %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
  %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
  %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After EmplaceAllocationsPass (iree-stream-emplace-allocations) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c24} -> !stream.resource<*>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c4} -> !stream.resource<*>{%c4}
  %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%1[%c0 to %c24 for %c24], %3[%c0 to %c4 for %c4]) : (!stream.resource<*>{%c24}, !stream.resource<*>{%c4}) -> !stream.resource<*>{%c24}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c24} -> !stream.resource<external>{%c24}
  %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After RefineUsagePass (iree-stream-refine-usage) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %2 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%0[%c0 to %c24 for %c24], %1[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %2 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%0[%c0 to %c24 for %c24], %1[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
  %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %2 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%0[%c0 to %c24 for %c24], %1[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
  %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %2 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%0[%c0 to %c24 for %c24], %1[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
  %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %2 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%0[%c0 to %c24 for %c24], %1[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %2 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%0[%c0 to %c24 for %c24], %1[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %2 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%0[%c0 to %c24 for %c24], %1[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %2 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%0[%c0 to %c24 for %c24], %1[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyAsyncAccessRangesPass (iree-stream-verify-async-access-ranges) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %2 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%0[%c0 to %c24 for %c24], %1[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleExecutionPass (iree-stream-schedule-execution) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    stream.yield %4 : !stream.resource<external>{%c24}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
  %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After ScheduleConcurrencyPass (iree-stream-schedule-concurrency) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    stream.yield %4 : !stream.resource<external>{%c24}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
  %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After PropagateTimepointsPass (iree-stream-propagate-timepoints) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %2 = stream.timepoint.immediate => !stream.timepoint
    %3 = stream.timepoint.immediate => !stream.timepoint
    %4 = stream.timepoint.join max(%2, %3) => !stream.timepoint
    %results, %result_timepoint = stream.async.execute await(%4) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
      %7 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
      stream.yield %7 : !stream.resource<external>{%c24}
    } => !stream.timepoint
    %5 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
    %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeBuiltinsPass (iree-stream-materialize-builtins) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %2 = stream.timepoint.immediate => !stream.timepoint
    %3 = stream.timepoint.immediate => !stream.timepoint
    %4 = stream.timepoint.join max(%2, %3) => !stream.timepoint
    %results, %result_timepoint = stream.async.execute await(%4) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
      %7 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
      stream.yield %7 : !stream.resource<external>{%c24}
    } => !stream.timepoint
    %5 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
    %6 = stream.tensor.export %5 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    stream.yield %4 : !stream.resource<external>{%c24}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
  %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    stream.yield %4 : !stream.resource<external>{%c24}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
  %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
    %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
    stream.yield %4 : !stream.resource<external>{%c24}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
  %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
      %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
      stream.yield %4 : !stream.resource<external>{%c24}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
      %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
      stream.yield %4 : !stream.resource<external>{%c24}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
      %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
      stream.yield %4 : !stream.resource<external>{%c24}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
      %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
      stream.yield %4 : !stream.resource<external>{%c24}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsyncPass (iree-stream-verify-lowering-to-async) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24} {
      %4 = stream.async.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%arg2[%c0 to %c24 for %c24], %arg3[%c0 to %c4 for %c4]) : (!stream.resource<external>{%c24}, !stream.resource<external>{%c4}) -> !stream.resource<external>{%c24}
      stream.yield %4 : !stream.resource<external>{%c24}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c24}
    %3 = stream.tensor.export %2 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleAllocationPass (iree-stream-schedule-allocation) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %c0_0 = arith.constant 0 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0_0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After PackConstantsPass (iree-stream-pack-constants) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %c0_0 = arith.constant 0 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0_0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After LayoutSlicesPass (iree-stream-layout-slices) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %c0_0 = arith.constant 0 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0_0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %c0_0 = arith.constant 0 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0_0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToCmdPass (iree-stream-verify-lowering-to-cmd) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ElideTimepointsPass (iree-stream-elide-timepoints) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseDispatchBindingsPass (iree-stream-fuse-dispatch-bindings) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: index, %arg4: index, %arg5: index) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0, %c0, %c0 : index, index, index) {
        ro %arg2[%c0_0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0_0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0_0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchArgumentsPass (iree-stream-annotate-dispatch-arguments) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.values = [0 : index]}, %arg5: index {stream.values = [0 : index]}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0, %c0, %c0 : index, index, index) {
        ro %arg2[%c0_0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0_0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0_0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After PackDispatchOperandsPass (iree-stream-pack-dispatch-operands) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %c32_i64 = arith.constant 32 : i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %c32_i64_0 = arith.constant 32 : i64
        %7 = arith.shli %6, %c32_i64_0 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %c32_i64_1 = arith.constant 32 : i64
        %12 = arith.shli %11, %c32_i64_1 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %c0 = arith.constant 0 : index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %19 = flow.dispatch.tensor.load %16, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %20 = tensor.empty() : tensor<6xf32>
        %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%18, %19 : tensor<6xf32>, tensor<f32>) outs(%20 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_2: f32, %out: f32):
          %22 = arith.addf %in, %in_2 : f32
          linalg.yield %22 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %21, %17, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %c0_i64 = arith.constant 0 : i64
    %c0_i32 = arith.constant 0 : i32
    %c32_i64 = arith.constant 32 : i64
    %c0_i64_1 = arith.constant 0 : i64
    %c0_i32_2 = arith.constant 0 : i32
    %c0_i64_3 = arith.constant 0 : i64
    %c0_i32_4 = arith.constant 0 : i32
    %c32_i64_5 = arith.constant 32 : i64
    %c0_i64_6 = arith.constant 0 : i64
    %c0_i32_7 = arith.constant 0 : i32
    %c0_i64_8 = arith.constant 0 : i64
    %c0_i32_9 = arith.constant 0 : i32
    %c32_i64_10 = arith.constant 32 : i64
    %c0_i64_11 = arith.constant 0 : i64
    %c0_i32_12 = arith.constant 0 : i32
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0_i32, %c0_i32_2, %c0_i32_4, %c0_i32_7, %c0_i32_9, %c0_i32_12 : i32, i32, i32, i32, i32, i32) {
        ro %arg2[%c0_0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0_0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0_0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %19 = flow.dispatch.tensor.load %16, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %20 = tensor.empty() : tensor<6xf32>
        %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%18, %19 : tensor<6xf32>, tensor<f32>) outs(%20 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %22 = arith.addf %in, %in_0 : f32
          linalg.yield %22 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %21, %17, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %19 = flow.dispatch.tensor.load %16, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %20 = tensor.empty() : tensor<6xf32>
        %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%18, %19 : tensor<6xf32>, tensor<f32>) outs(%20 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %22 = arith.addf %in, %in_0 : f32
          linalg.yield %22 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %21, %17, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %19 = flow.dispatch.tensor.load %16, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %20 = tensor.empty() : tensor<6xf32>
        %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%18, %19 : tensor<6xf32>, tensor<f32>) outs(%20 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %22 = arith.addf %in, %in_0 : f32
          linalg.yield %22 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %21, %17, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %19 = flow.dispatch.tensor.load %16, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %20 = tensor.empty() : tensor<6xf32>
        %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%18, %19 : tensor<6xf32>, tensor<f32>) outs(%20 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %22 = arith.addf %in, %in_0 : f32
          linalg.yield %22 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %21, %17, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldUniformOperandsPass (iree-stream-fold-uniform-operands) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %c0_i32 : i32 to i64
        %1 = arith.extui %c0_i32 : i32 to i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %c0_i32 : i32 to i64
        %6 = arith.extui %c0_i32 : i32 to i64
        %7 = arith.shli %6, %c32_i64 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %c0_i32 : i32 to i64
        %11 = arith.extui %c0_i32 : i32 to i64
        %12 = arith.shli %11, %c32_i64 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = stream.binding.subspan %arg0[%4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %16 = stream.binding.subspan %arg1[%9] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %17 = stream.binding.subspan %arg2[%14] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %18 = flow.dispatch.tensor.load %15, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %19 = flow.dispatch.tensor.load %16, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %20 = tensor.empty() : tensor<6xf32>
        %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%18, %19 : tensor<6xf32>, tensor<f32>) outs(%20 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %22 = arith.addf %in, %in_0 : f32
          linalg.yield %22 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %21, %17, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyTargetEnvironmentPass (iree-hal-verify-target-environment) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  stream.executable private @add_dispatch_0 {
    stream.executable.export public @add_dispatch_0_broadcast_6_f32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeInterfacesPass (iree-hal-materialize-interfaces) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]} {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @add_dispatch_0_broadcast_6_f32() {
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
          %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
          %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
          %5 = tensor.empty() : tensor<6xf32>
          %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
          ^bb0(%in: f32, %in_0: f32, %out: f32):
            %7 = arith.addf %in, %in_0 : f32
            linalg.yield %7 : f32
          } -> tensor<6xf32>
          flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
          return
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After PruneExecutablesPass (iree-hal-prune-executables) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0) -> (d0)>
#map1 = affine_map<(d0) -> ()>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]} {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @add_dispatch_0_broadcast_6_f32() {
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
          %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
          %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
          %5 = tensor.empty() : tensor<6xf32>
          %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
          ^bb0(%in: f32, %in_0: f32, %out: f32):
            %7 = arith.addf %in, %in_0 : f32
            linalg.yield %7 : f32
          } -> tensor<6xf32>
          flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
          return
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
      stream.cmd.dispatch @add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32 {
        ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
        ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
        wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
    %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After CPUMaterializeUpperBoundTileSize (iree-codegen-cpu-materialize-upper-bound-tile-size) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<3x2xf32> in !stream.resource<external>{%c24}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<1x1xf32> in !stream.resource<external>{%c4}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<external>{%c24} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg2: !stream.resource<external>{%c24}, %1 as %arg3: !stream.resource<external>{%c4}, %result as %arg4: !stream.resource<external>{%c24}) {
    stream.cmd.dispatch @add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32 {
      ro %arg2[%c0 for %c24] : !stream.resource<external>{%c24},
      ro %arg3[%c0 for %c4] : !stream.resource<external>{%c4},
      wo %arg4[%c0 for %c24] : !stream.resource<external>{%c24}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c24}
  %4 = stream.tensor.export %3 : tensor<3x2xf32> in !stream.resource<external>{%c24} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After TypePropagation (iree-codegen-type-propagation) //----- //
func.func @add_dispatch_0_broadcast_6_f32() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = tensor.empty() : tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After BubbleUpOrdinalOps (iree-codegen-bubble-up-ordinal-ops) //----- //
func.func @add_dispatch_0_broadcast_6_f32() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = tensor.empty() : tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After BufferizeCopyOnlyDispatches (iree-codegen-bufferize-copy-only-dispatches) //----- //
func.func @add_dispatch_0_broadcast_6_f32() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = tensor.empty() : tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After DecomposeSoftmax (iree-codegen-decompose-softmax) //----- //
func.func @add_dispatch_0_broadcast_6_f32() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = tensor.empty() : tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After MaterializeUserConfigs (iree-codegen-materialize-user-configs) //----- //
module {
  func.func @add_dispatch_0_broadcast_6_f32() {
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
    %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
    %5 = tensor.empty() : tensor<6xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<6xf32>
    flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    return
  }
}

// -----// IR Dump After RematerializeParallelOps (iree-codegen-rematerialize-parallel-ops) //----- //
func.func @add_dispatch_0_broadcast_6_f32() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = tensor.empty() : tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After ExpandF16OpToF32 (iree-llvmcpu-expand-f16-op-to-f32) //----- //
func.func @add_dispatch_0_broadcast_6_f32() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = tensor.empty() : tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After CPUMaterializeEncoding (iree-codegen-cpu-materialize-encoding) //----- //
func.func @add_dispatch_0_broadcast_6_f32() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = tensor.empty() : tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @add_dispatch_0_broadcast_6_f32() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = tensor.empty() : tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After LLVMCPUSelectLoweringStrategy (iree-llvmcpu-select-lowering-strategy) //----- //
module {
  func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
    %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
    %5 = tensor.empty() : tensor<6xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<6xf32>
    flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    return
  }
}

// -----// IR Dump After ConfigureTargetExecutableVariantsPass (iree-hal-configure-target-executable-variants) //----- //
hal.executable.variant public @embedded_elf_arm_64 target(<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>) {
  hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]} {
  ^bb0(%arg0: !hal.device):
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
      %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
      %5 = tensor.empty() : tensor<6xf32>
      %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
      ^bb0(%in: f32, %in_0: f32, %out: f32):
        %7 = arith.addf %in, %in_0 : f32
        linalg.yield %7 : f32
      } -> tensor<6xf32>
      flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
      return
    }
  }
}

// -----// IR Dump After ConfigureExecutablesPass (iree-hal-configure-executables) //----- //
hal.executable private @add_dispatch_0 {
  hal.executable.variant public @embedded_elf_arm_64 target(<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>) {
    hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]} {
    ^bb0(%arg0: !hal.device):
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      hal.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
        %c0 = arith.constant 0 : index
        %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
        %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
        %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
        %5 = tensor.empty() : tensor<6xf32>
        %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
        ^bb0(%in: f32, %in_0: f32, %out: f32):
          %7 = arith.addf %in, %in_0 : f32
          linalg.yield %7 : f32
        } -> tensor<6xf32>
        flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
        return
      }
    }
  }
}

// -----// IR Dump After LowerExecutableUsingTransformDialect (iree-codegen-lower-executable-using-transform-dialect) //----- //
module {
  func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
    %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
    %5 = tensor.empty() : tensor<6xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %4 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<6xf32>
    flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
    return
  }
}

// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %4 = flow.dispatch.tensor.load %0, offsets = [%c0], sizes = [%c6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<?xf32>
  %5 = tensor.empty() : tensor<6xf32>
  %cast = tensor.cast %4 : tensor<?xf32> to tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cast, %3 : tensor<6xf32>, tensor<f32>) outs(%5 : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %7 = arith.addf %in, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  %cast_0 = tensor.cast %6 : tensor<6xf32> to tensor<?xf32>
  flow.dispatch.tensor.store %cast_0, %2, offsets = [%c0], sizes = [%c6], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [%c0], sizes = [%c6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<?xf32>
  %cast = tensor.cast %3 : tensor<?xf32> to tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [%c0], sizes = [%c6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<?xf32>
  %cast_0 = tensor.cast %5 : tensor<?xf32> to tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cast_0, %4 : tensor<6xf32>, tensor<f32>) outs(%cast : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_2: f32, %out: f32):
    %7 = arith.addf %in, %in_2 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  %cast_1 = tensor.cast %6 : tensor<6xf32> to tensor<?xf32>
  flow.dispatch.tensor.store %cast_1, %2, offsets = [%c0], sizes = [%c6], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After FoldAffineMinInDistributedLoops (iree-codegen-fold-affinemin-in-distributed-loops) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [%c0], sizes = [%c6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<?xf32>
  %cast = tensor.cast %3 : tensor<?xf32> to tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [%c0], sizes = [%c6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<?xf32>
  %cast_0 = tensor.cast %5 : tensor<?xf32> to tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cast_0, %4 : tensor<6xf32>, tensor<f32>) outs(%cast : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_2: f32, %out: f32):
    %7 = arith.addf %in, %in_2 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  %cast_1 = tensor.cast %6 : tensor<6xf32> to tensor<?xf32>
  flow.dispatch.tensor.store %cast_1, %2, offsets = [%c0], sizes = [%c6], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5, %4 : tensor<6xf32>, tensor<f32>) outs(%3 : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5, %4 : tensor<6xf32>, tensor<f32>) outs(%3 : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5, %4 : tensor<6xf32>, tensor<f32>) outs(%3 : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5, %4 : tensor<6xf32>, tensor<f32>) outs(%3 : tensor<6xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<6xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After LLVMCPUTileAndFuse (iree-llvmcpu-tile-and-fuse) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = scf.for %arg0 = %c0 to %c6 step %c4 iter_args(%arg1 = %3) -> (tensor<6xf32>) {
    %7 = affine.min affine_map<(d0) -> (-d0 + 6, 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %5[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %extracted_slice_0 = tensor.extract_slice %arg1[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<?xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %9 = arith.addf %in, %in_1 : f32
      linalg.yield %9 : f32
    } -> tensor<?xf32>
    %inserted_slice = tensor.insert_slice %8 into %arg1[%arg0] [%7] [1] : tensor<?xf32> into tensor<6xf32>
    scf.yield %inserted_slice : tensor<6xf32>
  }
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = scf.for %arg0 = %c0 to %c6 step %c4 iter_args(%arg1 = %3) -> (tensor<6xf32>) {
    %7 = affine.min affine_map<(d0) -> (-d0 + 6, 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %5[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %extracted_slice_0 = tensor.extract_slice %arg1[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<?xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %9 = arith.addf %in, %in_1 : f32
      linalg.yield %9 : f32
    } -> tensor<?xf32>
    %inserted_slice = tensor.insert_slice %8 into %arg1[%arg0] [%7] [1] : tensor<?xf32> into tensor<6xf32>
    scf.yield %inserted_slice : tensor<6xf32>
  }
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = scf.for %arg0 = %c0 to %c6 step %c4 iter_args(%arg1 = %3) -> (tensor<6xf32>) {
    %7 = affine.min affine_map<(d0) -> (-d0 + 6, 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %5[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %extracted_slice_0 = tensor.extract_slice %arg1[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<?xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %9 = arith.addf %in, %in_1 : f32
      linalg.yield %9 : f32
    } -> tensor<?xf32>
    %inserted_slice = tensor.insert_slice %8 into %arg1[%arg0] [%7] [1] : tensor<?xf32> into tensor<6xf32>
    scf.yield %inserted_slice : tensor<6xf32>
  }
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After LLVMCPUSplitReduction (iree-llvmcpu-split-reduction) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = scf.for %arg0 = %c0 to %c6 step %c4 iter_args(%arg1 = %3) -> (tensor<6xf32>) {
    %7 = affine.min affine_map<(d0) -> (-d0 + 6, 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %5[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %extracted_slice_0 = tensor.extract_slice %arg1[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<?xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %9 = arith.addf %in, %in_1 : f32
      linalg.yield %9 : f32
    } -> tensor<?xf32>
    %inserted_slice = tensor.insert_slice %8 into %arg1[%arg0] [%7] [1] : tensor<?xf32> into tensor<6xf32>
    scf.yield %inserted_slice : tensor<6xf32>
  }
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After LLVMCPUTile (iree-llvmcpu-tile) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = scf.for %arg0 = %c0 to %c6 step %c4 iter_args(%arg1 = %3) -> (tensor<6xf32>) {
    %7 = affine.min affine_map<(d0) -> (-d0 + 6, 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %5[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %extracted_slice_0 = tensor.extract_slice %arg1[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<?xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %9 = arith.addf %in, %in_1 : f32
      linalg.yield %9 : f32
    } -> tensor<?xf32>
    %inserted_slice = tensor.insert_slice %8 into %arg1[%arg0] [%7] [1] : tensor<?xf32> into tensor<6xf32>
    scf.yield %inserted_slice : tensor<6xf32>
  }
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After LLVMCPUTileAndFuse (iree-llvmcpu-tile-and-fuse) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = scf.for %arg0 = %c0 to %c6 step %c4 iter_args(%arg1 = %3) -> (tensor<6xf32>) {
    %7 = affine.min affine_map<(d0) -> (-d0 + 6, 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %5[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %extracted_slice_0 = tensor.extract_slice %arg1[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<?xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %9 = arith.addf %in, %in_1 : f32
      linalg.yield %9 : f32
    } -> tensor<?xf32>
    %inserted_slice = tensor.insert_slice %8 into %arg1[%arg0] [%7] [1] : tensor<?xf32> into tensor<6xf32>
    scf.yield %inserted_slice : tensor<6xf32>
  }
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = scf.for %arg0 = %c0 to %c6 step %c4 iter_args(%arg1 = %3) -> (tensor<6xf32>) {
    %7 = affine.min affine_map<(d0) -> (-d0 + 6, 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %5[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %extracted_slice_0 = tensor.extract_slice %arg1[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<?xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %9 = arith.addf %in, %in_1 : f32
      linalg.yield %9 : f32
    } -> tensor<?xf32>
    %inserted_slice = tensor.insert_slice %8 into %arg1[%arg0] [%7] [1] : tensor<?xf32> into tensor<6xf32>
    scf.yield %inserted_slice : tensor<6xf32>
  }
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c6 = arith.constant 6 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = scf.for %arg0 = %c0 to %c6 step %c4 iter_args(%arg1 = %3) -> (tensor<6xf32>) {
    %7 = affine.min affine_map<(d0) -> (-d0 + 6, 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %5[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %extracted_slice_0 = tensor.extract_slice %arg1[%arg0] [%7] [1] : tensor<6xf32> to tensor<?xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<?xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %9 = arith.addf %in, %in_1 : f32
      linalg.yield %9 : f32
    } -> tensor<?xf32>
    %inserted_slice = tensor.insert_slice %8 into %arg1[%arg0] [%7] [1] : tensor<?xf32> into tensor<6xf32>
    scf.yield %inserted_slice : tensor<6xf32>
  }
  flow.dispatch.tensor.store %6, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After LLVMCPUPeel (iree-llvmcpu-peel) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %extracted_slice = tensor.extract_slice %5[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %extracted_slice_0 = tensor.extract_slice %3[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<4xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<4xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<4xf32>
  %inserted_slice = tensor.insert_slice %6 into %3[0] [4] [1] : tensor<4xf32> into tensor<6xf32>
  %extracted_slice_1 = tensor.extract_slice %5[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %extracted_slice_2 = tensor.extract_slice %inserted_slice[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice_1, %4 : tensor<2xf32>, tensor<f32>) outs(%extracted_slice_2 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<2xf32>
  %inserted_slice_3 = tensor.insert_slice %7 into %inserted_slice[4] [2] [1] : tensor<2xf32> into tensor<6xf32>
  flow.dispatch.tensor.store %inserted_slice_3, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After TensorToVectorVectorizePad (iree-codegen-vectorize-tensor-pad) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %extracted_slice = tensor.extract_slice %5[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %extracted_slice_0 = tensor.extract_slice %3[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<4xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<4xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<4xf32>
  %inserted_slice = tensor.insert_slice %6 into %3[0] [4] [1] : tensor<4xf32> into tensor<6xf32>
  %extracted_slice_1 = tensor.extract_slice %5[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %extracted_slice_2 = tensor.extract_slice %inserted_slice[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice_1, %4 : tensor<2xf32>, tensor<f32>) outs(%extracted_slice_2 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<2xf32>
  %inserted_slice_3 = tensor.insert_slice %7 into %inserted_slice[4] [2] [1] : tensor<2xf32> into tensor<6xf32>
  flow.dispatch.tensor.store %inserted_slice_3, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After DecomposePackUnPackOps (iree-codegen-decompose-pack-unpack-ops) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %extracted_slice = tensor.extract_slice %5[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %extracted_slice_0 = tensor.extract_slice %3[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<4xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<4xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<4xf32>
  %inserted_slice = tensor.insert_slice %6 into %3[0] [4] [1] : tensor<4xf32> into tensor<6xf32>
  %extracted_slice_1 = tensor.extract_slice %5[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %extracted_slice_2 = tensor.extract_slice %inserted_slice[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice_1, %4 : tensor<2xf32>, tensor<f32>) outs(%extracted_slice_2 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<2xf32>
  %inserted_slice_3 = tensor.insert_slice %7 into %inserted_slice[4] [2] [1] : tensor<2xf32> into tensor<6xf32>
  flow.dispatch.tensor.store %inserted_slice_3, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %extracted_slice = tensor.extract_slice %5[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %extracted_slice_0 = tensor.extract_slice %3[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<4xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<4xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<4xf32>
  %inserted_slice = tensor.insert_slice %6 into %3[0] [4] [1] : tensor<4xf32> into tensor<6xf32>
  %extracted_slice_1 = tensor.extract_slice %5[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %extracted_slice_2 = tensor.extract_slice %inserted_slice[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice_1, %4 : tensor<2xf32>, tensor<f32>) outs(%extracted_slice_2 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<2xf32>
  %inserted_slice_3 = tensor.insert_slice %7 into %inserted_slice[4] [2] [1] : tensor<2xf32> into tensor<6xf32>
  flow.dispatch.tensor.store %inserted_slice_3, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %extracted_slice = tensor.extract_slice %5[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %extracted_slice_0 = tensor.extract_slice %3[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %4 : tensor<4xf32>, tensor<f32>) outs(%extracted_slice_0 : tensor<4xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<4xf32>
  %inserted_slice = tensor.insert_slice %6 into %3[0] [4] [1] : tensor<4xf32> into tensor<6xf32>
  %extracted_slice_1 = tensor.extract_slice %5[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %extracted_slice_2 = tensor.extract_slice %inserted_slice[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice_1, %4 : tensor<2xf32>, tensor<f32>) outs(%extracted_slice_2 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[6], [4], [0], [0]]>} {
  ^bb0(%in: f32, %in_4: f32, %out: f32):
    %8 = arith.addf %in, %in_4 : f32
    linalg.yield %8 : f32
  } -> tensor<2xf32>
  %inserted_slice_3 = tensor.insert_slice %7 into %inserted_slice[4] [2] [1] : tensor<2xf32> into tensor<6xf32>
  flow.dispatch.tensor.store %inserted_slice_3, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After GenericVectorization (iree-codegen-generic-vectorization) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %extracted_slice = tensor.extract_slice %5[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %extracted_slice_0 = tensor.extract_slice %3[0] [4] [1] : tensor<6xf32> to tensor<4xf32>
  %6 = vector.transfer_read %extracted_slice[%c0], %cst {in_bounds = [true]} : tensor<4xf32>, vector<4xf32>
  %extracted = tensor.extract %4[] : tensor<f32>
  %7 = vector.broadcast %extracted : f32 to vector<4xf32>
  %8 = arith.addf %6, %7 : vector<4xf32>
  %9 = vector.transfer_write %8, %extracted_slice_0[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<4xf32>
  %inserted_slice = tensor.insert_slice %9 into %3[0] [4] [1] : tensor<4xf32> into tensor<6xf32>
  %extracted_slice_1 = tensor.extract_slice %5[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %extracted_slice_2 = tensor.extract_slice %inserted_slice[4] [2] [1] : tensor<6xf32> to tensor<2xf32>
  %10 = vector.transfer_read %extracted_slice_1[%c0], %cst {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
  %extracted_3 = tensor.extract %4[] : tensor<f32>
  %11 = vector.broadcast %extracted_3 : f32 to vector<2xf32>
  %12 = arith.addf %10, %11 : vector<2xf32>
  %13 = vector.transfer_write %12, %extracted_slice_2[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
  %inserted_slice_4 = tensor.insert_slice %13 into %inserted_slice[4] [2] [1] : tensor<2xf32> into tensor<6xf32>
  flow.dispatch.tensor.store %inserted_slice_4, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After OptimizeTensorInsertExtractSlices (iree-codegen-optimize-tensor-insert-extract-slices) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = vector.transfer_read %5[%c0], %cst {in_bounds = [true]} : tensor<6xf32>, vector<4xf32>
  %extracted = tensor.extract %4[] : tensor<f32>
  %7 = vector.broadcast %extracted : f32 to vector<4xf32>
  %8 = arith.addf %6, %7 : vector<4xf32>
  %9 = vector.transfer_write %8, %3[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<6xf32>
  %10 = vector.transfer_read %5[%c4], %cst {in_bounds = [true]} : tensor<6xf32>, vector<2xf32>
  %extracted_0 = tensor.extract %4[] : tensor<f32>
  %11 = vector.broadcast %extracted_0 : f32 to vector<2xf32>
  %12 = arith.addf %10, %11 : vector<2xf32>
  %13 = vector.transfer_write %12, %9[%c4] {in_bounds = [true]} : vector<2xf32>, tensor<6xf32>
  flow.dispatch.tensor.store %13, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = vector.transfer_read %5[%c0], %cst {in_bounds = [true]} : tensor<6xf32>, vector<4xf32>
  %extracted = tensor.extract %4[] : tensor<f32>
  %7 = vector.broadcast %extracted : f32 to vector<4xf32>
  %8 = arith.addf %6, %7 : vector<4xf32>
  %9 = vector.transfer_write %8, %3[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<6xf32>
  %10 = vector.transfer_read %5[%c4], %cst {in_bounds = [true]} : tensor<6xf32>, vector<2xf32>
  %extracted_0 = tensor.extract %4[] : tensor<f32>
  %11 = vector.broadcast %extracted_0 : f32 to vector<2xf32>
  %12 = arith.addf %10, %11 : vector<2xf32>
  %13 = vector.transfer_write %12, %9[%c4] {in_bounds = [true]} : vector<2xf32>, tensor<6xf32>
  flow.dispatch.tensor.store %13, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = vector.transfer_read %5[%c0], %cst {in_bounds = [true]} : tensor<6xf32>, vector<4xf32>
  %extracted = tensor.extract %4[] : tensor<f32>
  %7 = vector.broadcast %extracted : f32 to vector<4xf32>
  %8 = arith.addf %6, %7 : vector<4xf32>
  %9 = vector.transfer_write %8, %3[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<6xf32>
  %10 = vector.transfer_read %5[%c4], %cst {in_bounds = [true]} : tensor<6xf32>, vector<2xf32>
  %11 = vector.broadcast %extracted : f32 to vector<2xf32>
  %12 = arith.addf %10, %11 : vector<2xf32>
  %13 = vector.transfer_write %12, %9[%c4] {in_bounds = [true]} : vector<2xf32>, tensor<6xf32>
  flow.dispatch.tensor.store %13, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After LLVMCPUVerifyVectorSizeLegality (iree-llvmcpu-verify-vector-size-legality) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = vector.transfer_read %5[%c0], %cst {in_bounds = [true]} : tensor<6xf32>, vector<4xf32>
  %extracted = tensor.extract %4[] : tensor<f32>
  %7 = vector.broadcast %extracted : f32 to vector<4xf32>
  %8 = arith.addf %6, %7 : vector<4xf32>
  %9 = vector.transfer_write %8, %3[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<6xf32>
  %10 = vector.transfer_read %5[%c4], %cst {in_bounds = [true]} : tensor<6xf32>, vector<2xf32>
  %11 = vector.broadcast %extracted : f32 to vector<2xf32>
  %12 = arith.addf %10, %11 : vector<2xf32>
  %13 = vector.transfer_write %12, %9[%c4] {in_bounds = [true]} : vector<2xf32>, tensor<6xf32>
  flow.dispatch.tensor.store %13, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = vector.transfer_read %5[%c0], %cst {in_bounds = [true]} : tensor<6xf32>, vector<4xf32>
  %extracted = tensor.extract %4[] : tensor<f32>
  %7 = vector.broadcast %extracted : f32 to vector<4xf32>
  %8 = arith.addf %6, %7 : vector<4xf32>
  %9 = vector.transfer_write %8, %3[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<6xf32>
  %10 = vector.transfer_read %5[%c4], %cst {in_bounds = [true]} : tensor<6xf32>, vector<2xf32>
  %11 = vector.broadcast %extracted : f32 to vector<2xf32>
  %12 = arith.addf %10, %11 : vector<2xf32>
  %13 = vector.transfer_write %12, %9[%c4] {in_bounds = [true]} : vector<2xf32>, tensor<6xf32>
  flow.dispatch.tensor.store %13, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<6xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<f32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  %3 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<6xf32>> -> tensor<6xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [], sizes = [], strides = [] : !flow.dispatch.tensor<readonly:tensor<f32>> -> tensor<f32>
  %5 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [6], strides = [1] : !flow.dispatch.tensor<readonly:tensor<6xf32>> -> tensor<6xf32>
  %6 = vector.transfer_read %5[%c0], %cst {in_bounds = [true]} : tensor<6xf32>, vector<4xf32>
  %extracted = tensor.extract %4[] : tensor<f32>
  %7 = vector.broadcast %extracted : f32 to vector<4xf32>
  %8 = arith.addf %6, %7 : vector<4xf32>
  %9 = vector.transfer_write %8, %3[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<6xf32>
  %10 = vector.transfer_read %5[%c4], %cst {in_bounds = [true]} : tensor<6xf32>, vector<2xf32>
  %11 = vector.broadcast %extracted : f32 to vector<2xf32>
  %12 = arith.addf %10, %11 : vector<2xf32>
  %13 = vector.transfer_write %12, %9[%c4] {in_bounds = [true]} : vector<2xf32>, tensor<6xf32>
  flow.dispatch.tensor.store %13, %2, offsets = [0], sizes = [6], strides = [1] : tensor<6xf32> -> !flow.dispatch.tensor<writeonly:tensor<6xf32>>
  return
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : memref<6xf32, #hal.descriptor_type<storage_buffer>>) outs(%2 : memref<6xf32, #hal.descriptor_type<storage_buffer>>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  }
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : memref<6xf32, #hal.descriptor_type<storage_buffer>>) outs(%2 : memref<6xf32, #hal.descriptor_type<storage_buffer>>) {
  ^bb0(%in: f32, %out: f32):
    linalg.yield %in : f32
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After LLVMCPUDropVectorUnitDims (iree-llvmcpu-drop-vector-unit-dims) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After LLVMCPUVirtualVectorLowering (iree-llvmcpu-virtual-vector-lowering) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.transfer_write %6, %2[%c0] {in_bounds = [true]} : vector<4xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %7 = vector.transfer_read %0[%c4], %cst {in_bounds = [true]} : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.transfer_write %9, %2[%c4] {in_bounds = [true]} : vector<2xf32>, memref<6xf32, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After LLVMCPUVectorTransferLowering (iree-llvmcpu-vector-transfer-lowering) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.load %0[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  return
}

// -----// IR Dump After LLVMCPUVectorTransposeLowering (iree-llvmcpu-vector-transpose-lowering) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.load %0[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.load %0[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  return
}

// -----// IR Dump After LLVMCPUVectorShapeCastLowering (iree-llvmcpu-vector-shape-cast-lowering) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.load %0[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  return
}

// -----// IR Dump After LLVMCPULowerExecutableTarget (iree-llvmcpu-lower-executable-target) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<f32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<6xf32, #hal.descriptor_type<storage_buffer>>
  %3 = vector.load %0[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32, #hal.descriptor_type<storage_buffer>>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After LowerUKernelOpsToCalls (iree-codegen-lower-ukernel-ops-to-calls) //----- //
module {
  func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
    memref.assume_alignment %0, 64 : memref<6xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
    memref.assume_alignment %1, 64 : memref<f32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
    memref.assume_alignment %2, 64 : memref<6xf32>
    %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
    %4 = memref.load %1[] : memref<f32>
    %5 = vector.broadcast %4 : f32 to vector<4xf32>
    %6 = arith.addf %3, %5 : vector<4xf32>
    vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
    %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
    %8 = vector.broadcast %4 : f32 to vector<2xf32>
    %9 = arith.addf %7, %8 : vector<2xf32>
    vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
    return
  }
}

// -----// IR Dump After LinalgExtToLoops (iree-linalg-ext-to-loops) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ConvertLinalgToLoopsPass (convert-linalg-to-loops) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ConvertBf16ArithToF32 (iree-convert-bf16-arith-to-f32) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ConvertBf16ToUInt16Buffers (iree-convert-bf16-to-uint16-buffers) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After OneShotBufferize (one-shot-bufferize) //----- //
module {
  func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
    memref.assume_alignment %0, 64 : memref<6xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
    memref.assume_alignment %1, 64 : memref<f32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
    memref.assume_alignment %2, 64 : memref<6xf32>
    %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
    %4 = memref.load %1[] : memref<f32>
    %5 = vector.broadcast %4 : f32 to vector<4xf32>
    %6 = arith.addf %3, %5 : vector<4xf32>
    vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
    %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
    %8 = vector.broadcast %4 : f32 to vector<2xf32>
    %9 = arith.addf %7, %8 : vector<2xf32>
    vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
    return
  }
}

// -----// IR Dump After FoldTensorExtractOp (iree-codegen-fold-tensor-extract-op) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ConvertComplexToStandard (convert-complex-to-standard) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After PolynomialApproximationPass (iree-codegen-polynomial-approximation) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After HoistStaticallyBoundAllocations (iree-hoist-statically-bound-allocations) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After IREEExpandStridedMetadata (iree-codegen-expand-strided-metadata) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After LLVMCPUCheckIRBeforeLLVMConversion (iree-llvmcpu-check-ir-before-llvm-conversion) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ArithExpandOpsPass (arith-expand) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After EmulateNarrowType (iree-codegen-emulate-narrow-type) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @add_dispatch_0_broadcast_6_f32() attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<6xf32>
  memref.assume_alignment %0, 64 : memref<6xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<f32>
  memref.assume_alignment %1, 64 : memref<f32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<6xf32>
  memref.assume_alignment %2, 64 : memref<6xf32>
  %3 = vector.load %0[%c0] : memref<6xf32>, vector<4xf32>
  %4 = memref.load %1[] : memref<f32>
  %5 = vector.broadcast %4 : f32 to vector<4xf32>
  %6 = arith.addf %3, %5 : vector<4xf32>
  vector.store %6, %2[%c0] : memref<6xf32>, vector<4xf32>
  %7 = vector.load %0[%c4] : memref<6xf32>, vector<2xf32>
  %8 = vector.broadcast %4 : f32 to vector<2xf32>
  %9 = arith.addf %7, %8 : vector<2xf32>
  vector.store %9, %2[%c4] : memref<6xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ConvertToLLVM (iree-convert-to-llvm) //----- //
module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
  llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
    %0 = llvm.mlir.undef : vector<2xf32>
    %1 = llvm.mlir.constant(0 : i32) : i32
    %2 = llvm.mlir.undef : vector<4xf32>
    %3 = llvm.mlir.constant(63 : index) : i64
    %4 = llvm.mlir.constant(0 : index) : i64
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
    %9 = llvm.and %8, %3  : i64
    %10 = llvm.icmp "eq" %9, %4 : i64
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
    %16 = llvm.and %15, %3  : i64
    %17 = llvm.icmp "eq" %16, %4 : i64
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
    %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
    %23 = llvm.and %22, %3  : i64
    %24 = llvm.icmp "eq" %23, %4 : i64
    "llvm.intr.assume"(%24) : (i1) -> ()
    %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
    %26 = llvm.load %14 : !llvm.ptr -> f32
    %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
    %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
    %29 = llvm.fadd %25, %28  : vector<4xf32>
    llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
    %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
    %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
    %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
    %34 = llvm.fadd %31, %33  : vector<2xf32>
    %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
    llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %1 : i32
  }
}

// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
  llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
    %0 = llvm.mlir.undef : vector<2xf32>
    %1 = llvm.mlir.constant(0 : i32) : i32
    %2 = llvm.mlir.undef : vector<4xf32>
    %3 = llvm.mlir.constant(63 : index) : i64
    %4 = llvm.mlir.constant(0 : index) : i64
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
    %9 = llvm.and %8, %3  : i64
    %10 = llvm.icmp "eq" %9, %4 : i64
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
    %16 = llvm.and %15, %3  : i64
    %17 = llvm.icmp "eq" %16, %4 : i64
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
    %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
    %23 = llvm.and %22, %3  : i64
    %24 = llvm.icmp "eq" %23, %4 : i64
    "llvm.intr.assume"(%24) : (i1) -> ()
    %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
    %26 = llvm.load %14 : !llvm.ptr -> f32
    %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
    %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
    %29 = llvm.fadd %25, %28  : vector<4xf32>
    llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
    %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
    %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
    %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
    %34 = llvm.fadd %31, %33  : vector<2xf32>
    %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
    llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %1 : i32
  }
}

// -----// IR Dump After LLVMCPUSynchronizeSymbolVisibility (iree-llvmcpu-synchronize-symbol-visibility) //----- //
module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
  llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
    %0 = llvm.mlir.undef : vector<2xf32>
    %1 = llvm.mlir.constant(0 : i32) : i32
    %2 = llvm.mlir.undef : vector<4xf32>
    %3 = llvm.mlir.constant(63 : index) : i64
    %4 = llvm.mlir.constant(0 : index) : i64
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
    %9 = llvm.and %8, %3  : i64
    %10 = llvm.icmp "eq" %9, %4 : i64
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
    %16 = llvm.and %15, %3  : i64
    %17 = llvm.icmp "eq" %16, %4 : i64
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
    %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
    %23 = llvm.and %22, %3  : i64
    %24 = llvm.icmp "eq" %23, %4 : i64
    "llvm.intr.assume"(%24) : (i1) -> ()
    %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
    %26 = llvm.load %14 : !llvm.ptr -> f32
    %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
    %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
    %29 = llvm.fadd %25, %28  : vector<4xf32>
    llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
    %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
    %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
    %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
    %34 = llvm.fadd %31, %33  : vector<2xf32>
    %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
    llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %1 : i32
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
  llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
    %0 = llvm.mlir.undef : vector<2xf32>
    %1 = llvm.mlir.constant(0 : i32) : i32
    %2 = llvm.mlir.undef : vector<4xf32>
    %3 = llvm.mlir.constant(63 : index) : i64
    %4 = llvm.mlir.constant(0 : index) : i64
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
    %9 = llvm.and %8, %3  : i64
    %10 = llvm.icmp "eq" %9, %4 : i64
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
    %16 = llvm.and %15, %3  : i64
    %17 = llvm.icmp "eq" %16, %4 : i64
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
    %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
    %23 = llvm.and %22, %3  : i64
    %24 = llvm.icmp "eq" %23, %4 : i64
    "llvm.intr.assume"(%24) : (i1) -> ()
    %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
    %26 = llvm.load %14 : !llvm.ptr -> f32
    %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
    %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
    %29 = llvm.fadd %25, %28  : vector<4xf32>
    llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
    %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
    %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
    %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
    %34 = llvm.fadd %31, %33  : vector<2xf32>
    %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
    llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %1 : i32
  }
}

// -----// IR Dump After CSE (cse) //----- //
module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
  llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
    %0 = llvm.mlir.undef : vector<2xf32>
    %1 = llvm.mlir.constant(0 : i32) : i32
    %2 = llvm.mlir.undef : vector<4xf32>
    %3 = llvm.mlir.constant(63 : index) : i64
    %4 = llvm.mlir.constant(0 : index) : i64
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
    %9 = llvm.and %8, %3  : i64
    %10 = llvm.icmp "eq" %9, %4 : i64
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
    %16 = llvm.and %15, %3  : i64
    %17 = llvm.icmp "eq" %16, %4 : i64
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
    %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
    %23 = llvm.and %22, %3  : i64
    %24 = llvm.icmp "eq" %23, %4 : i64
    "llvm.intr.assume"(%24) : (i1) -> ()
    %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
    %26 = llvm.load %14 : !llvm.ptr -> f32
    %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
    %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
    %29 = llvm.fadd %25, %28  : vector<4xf32>
    llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
    %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
    %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
    %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
    %34 = llvm.fadd %31, %33  : vector<2xf32>
    %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
    llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %1 : i32
  }
}

// -----// IR Dump After AddFastMathFlags (iree-codegen-add-fast-math-flags) //----- //
llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
  %0 = llvm.mlir.undef : vector<2xf32>
  %1 = llvm.mlir.constant(0 : i32) : i32
  %2 = llvm.mlir.undef : vector<4xf32>
  %3 = llvm.mlir.constant(63 : index) : i64
  %4 = llvm.mlir.constant(0 : index) : i64
  %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
  %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
  %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
  %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
  %9 = llvm.and %8, %3  : i64
  %10 = llvm.icmp "eq" %9, %4 : i64
  "llvm.intr.assume"(%10) : (i1) -> ()
  %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
  %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
  %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
  %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
  %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
  %16 = llvm.and %15, %3  : i64
  %17 = llvm.icmp "eq" %16, %4 : i64
  "llvm.intr.assume"(%17) : (i1) -> ()
  %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
  %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
  %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
  %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
  %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
  %23 = llvm.and %22, %3  : i64
  %24 = llvm.icmp "eq" %23, %4 : i64
  "llvm.intr.assume"(%24) : (i1) -> ()
  %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
  %26 = llvm.load %14 : !llvm.ptr -> f32
  %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
  %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
  %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
  llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
  %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
  %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
  %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
  %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
  %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
  %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
  llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
  llvm.return %1 : i32
}

// -----// IR Dump After TranslateTargetExecutableVariantsPass (iree-hal-translate-target-executable-variants) //----- //
hal.executable.variant public @embedded_elf_arm_64 target(<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>) {
  hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
    llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
      %0 = llvm.mlir.undef : vector<2xf32>
      %1 = llvm.mlir.constant(0 : i32) : i32
      %2 = llvm.mlir.undef : vector<4xf32>
      %3 = llvm.mlir.constant(63 : index) : i64
      %4 = llvm.mlir.constant(0 : index) : i64
      %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
      %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
      %9 = llvm.and %8, %3  : i64
      %10 = llvm.icmp "eq" %9, %4 : i64
      "llvm.intr.assume"(%10) : (i1) -> ()
      %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
      %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
      %16 = llvm.and %15, %3  : i64
      %17 = llvm.icmp "eq" %16, %4 : i64
      "llvm.intr.assume"(%17) : (i1) -> ()
      %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
      %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
      %23 = llvm.and %22, %3  : i64
      %24 = llvm.icmp "eq" %23, %4 : i64
      "llvm.intr.assume"(%24) : (i1) -> ()
      %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
      %26 = llvm.load %14 : !llvm.ptr -> f32
      %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
      %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
      %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
      llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
      %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
      %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
      %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
      %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
      %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
      %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
      llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
      llvm.return %1 : i32
    }
  }
}

// -----// IR Dump After TranslateExecutablesPass (iree-hal-translate-executables) //----- //
hal.executable private @add_dispatch_0 {
  hal.executable.variant public @embedded_elf_arm_64 target(<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>) {
    hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]} {
    ^bb0(%arg0: !hal.device):
      %c1 = arith.constant 1 : index
      hal.return %c1, %c1, %c1 : index, index, index
    }
    builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
      llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
        %0 = llvm.mlir.undef : vector<2xf32>
        %1 = llvm.mlir.constant(0 : i32) : i32
        %2 = llvm.mlir.undef : vector<4xf32>
        %3 = llvm.mlir.constant(63 : index) : i64
        %4 = llvm.mlir.constant(0 : index) : i64
        %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
        %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
        %9 = llvm.and %8, %3  : i64
        %10 = llvm.icmp "eq" %9, %4 : i64
        "llvm.intr.assume"(%10) : (i1) -> ()
        %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
        %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
        %16 = llvm.and %15, %3  : i64
        %17 = llvm.icmp "eq" %16, %4 : i64
        "llvm.intr.assume"(%17) : (i1) -> ()
        %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
        %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
        %23 = llvm.and %22, %3  : i64
        %24 = llvm.icmp "eq" %23, %4 : i64
        "llvm.intr.assume"(%24) : (i1) -> ()
        %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
        %26 = llvm.load %14 : !llvm.ptr -> f32
        %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
        %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
        %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
        llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
        %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
        %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
        %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
        %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
        %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
        %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
        llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
        llvm.return %1 : i32
      }
    }
  }
}

// -----// IR Dump After ConvertToHALPass (iree-hal-conversion) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %c0_0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0_0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %c0_2 = arith.constant 0 : index
    %device_0_3 = hal.devices.get %c0_2 : !hal.device
    %allocator_4 = hal.device.allocator<%device_0_3 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator_4 : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %c0_5 = arith.constant 0 : index
    %device_0_6 = hal.devices.get %c0_5 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0_6 : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %transient_buffer = hal.device.queue.alloca<%device_0_6 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %c0_7 = arith.constant 0 : index
    %device_0_8 = hal.devices.get %c0_7 : !hal.device
    %c-1_i64_9 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_0_8 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %ok, %value = hal.device.query<%1 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %c0_10 = arith.constant 0 : index
    %2 = arith.select %value, %c0_10, %c-1 : index
    scf.index_switch %2 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%1 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_17 = arith.constant 0 : index
      %c1_18 = arith.constant 1 : index
      %c2_19 = arith.constant 2 : index
      %c0_20 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_20] bindings([
        %c0_17 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1_18 = (%buffer_1 : !hal.buffer)[%c0, %c4], 
        %c2_19 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %c1_21 = arith.constant 1 : index
      %exe = hal.executable.lookup device(%1 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1_21, %c1_21, %c1_21])
      scf.yield
    }
    default {
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_11 = hal.fence.create device(%device_0_8 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0_8 : !hal.device> affinity(%c-1_i64_9) wait(%fence) signal(%fence_11) commands([%cmd])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %dense_row_major_12 = hal.encoding_type<dense_row_major> : i32
    %element_type_f32_13 = hal.element_type<f32> : i32
    %c3_14 = arith.constant 3 : index
    %c2_15 = arith.constant 2 : index
    %c0_16 = arith.constant 0 : index
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0_16, %c24] shape([%c3_14, %c2_15]) type(%element_type_f32_13) encoding(%dense_row_major_12) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FixupLegacySyncPass (iree-hal-fixup-legacy-sync) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %c0_0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0_0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %c0_2 = arith.constant 0 : index
    %device_0_3 = hal.devices.get %c0_2 : !hal.device
    %allocator_4 = hal.device.allocator<%device_0_3 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator_4 : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %c0_5 = arith.constant 0 : index
    %device_0_6 = hal.devices.get %c0_5 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0_6 : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %transient_buffer = hal.device.queue.alloca<%device_0_6 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %c0_7 = arith.constant 0 : index
    %device_0_8 = hal.devices.get %c0_7 : !hal.device
    %c-1_i64_9 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_0_8 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %ok, %value = hal.device.query<%1 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %c0_10 = arith.constant 0 : index
    %2 = arith.select %value, %c0_10, %c-1 : index
    scf.index_switch %2 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%1 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_17 = arith.constant 0 : index
      %c1_18 = arith.constant 1 : index
      %c2_19 = arith.constant 2 : index
      %c0_20 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_20] bindings([
        %c0_17 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1_18 = (%buffer_1 : !hal.buffer)[%c0, %c4], 
        %c2_19 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %c1_21 = arith.constant 1 : index
      %exe = hal.executable.lookup device(%1 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1_21, %c1_21, %c1_21])
      scf.yield
    }
    default {
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_11 = hal.fence.create device(%device_0_8 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0_8 : !hal.device> affinity(%c-1_i64_9) wait(%fence) signal(%fence_11) commands([%cmd])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %dense_row_major_12 = hal.encoding_type<dense_row_major> : i32
    %element_type_f32_13 = hal.element_type<f32> : i32
    %c3_14 = arith.constant 3 : index
    %c2_15 = arith.constant 2 : index
    %c0_16 = arith.constant 0 : index
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0_16, %c24] shape([%c3_14, %c2_15]) type(%element_type_f32_13) encoding(%dense_row_major_12) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After PruneExecutablesPass (iree-hal-prune-executables) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %c0_0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0_0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_1 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %c0_2 = arith.constant 0 : index
    %device_0_3 = hal.devices.get %c0_2 : !hal.device
    %allocator_4 = hal.device.allocator<%device_0_3 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator_4 : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %c0_5 = arith.constant 0 : index
    %device_0_6 = hal.devices.get %c0_5 : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0_6 : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %transient_buffer = hal.device.queue.alloca<%device_0_6 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %c0_7 = arith.constant 0 : index
    %device_0_8 = hal.devices.get %c0_7 : !hal.device
    %c-1_i64_9 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_0_8 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %ok, %value = hal.device.query<%1 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %c0_10 = arith.constant 0 : index
    %2 = arith.select %value, %c0_10, %c-1 : index
    scf.index_switch %2 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%1 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_17 = arith.constant 0 : index
      %c1_18 = arith.constant 1 : index
      %c2_19 = arith.constant 2 : index
      %c0_20 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_20] bindings([
        %c0_17 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1_18 = (%buffer_1 : !hal.buffer)[%c0, %c4], 
        %c2_19 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %c1_21 = arith.constant 1 : index
      %exe = hal.executable.lookup device(%1 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1_21, %c1_21, %c1_21])
      scf.yield
    }
    default {
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_11 = hal.fence.create device(%device_0_8 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0_8 : !hal.device> affinity(%c-1_i64_9) wait(%fence) signal(%fence_11) commands([%cmd])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence_11]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %dense_row_major_12 = hal.encoding_type<dense_row_major> : i32
    %element_type_f32_13 = hal.element_type<f32> : i32
    %c3_14 = arith.constant 3 : index
    %c2_15 = arith.constant 2 : index
    %c0_16 = arith.constant 0 : index
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0_16, %c24] shape([%c3_14, %c2_15]) type(%element_type_f32_13) encoding(%dense_row_major_12) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %ok, %value = hal.device.query<%1 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %2 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %2 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%1 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%1 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %1 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    scf.index_switch %1 
    case 0 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
      scf.yield
    }
    default {
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %1 = arith.select %value, %c0, %c-1 : index
  scf.index_switch %1 
  case 0 {
    %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
    %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
    scf.yield
  }
  default {
  }
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After LLVMCPULinkExecutables (iree-llvmcpu-link-executables) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
hal.executable private @add_dispatch_0 {
  hal.executable.variant public @embedded_elf_arm_64 target(<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>) {
    hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) {
    ^bb0(%arg0: !hal.device):
      %c1 = arith.constant 1 : index
      hal.return %c1, %c1, %c1 : index, index, index
    }
    builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
      llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
        %0 = llvm.mlir.undef : vector<2xf32>
        %1 = llvm.mlir.constant(0 : i32) : i32
        %2 = llvm.mlir.undef : vector<4xf32>
        %3 = llvm.mlir.constant(63 : index) : i64
        %4 = llvm.mlir.constant(0 : index) : i64
        %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
        %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
        %9 = llvm.and %8, %3  : i64
        %10 = llvm.icmp "eq" %9, %4 : i64
        "llvm.intr.assume"(%10) : (i1) -> ()
        %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
        %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
        %16 = llvm.and %15, %3  : i64
        %17 = llvm.icmp "eq" %16, %4 : i64
        "llvm.intr.assume"(%17) : (i1) -> ()
        %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
        %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
        %23 = llvm.and %22, %3  : i64
        %24 = llvm.icmp "eq" %23, %4 : i64
        "llvm.intr.assume"(%24) : (i1) -> ()
        %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
        %26 = llvm.load %14 : !llvm.ptr -> f32
        %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
        %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
        %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
        llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
        %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
        %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
        %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
        %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
        %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
        %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
        llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
        llvm.return %1 : i32
      }
    }
  }
}

// -----// IR Dump After LLVMCPUAssignConstantOrdinals (iree-llvmcpu-assign-constant-ordinals) //----- //
hal.executable.variant public @embedded_elf_arm_64 target(<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>) {
  hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
    llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
      %0 = llvm.mlir.undef : vector<2xf32>
      %1 = llvm.mlir.constant(0 : i32) : i32
      %2 = llvm.mlir.undef : vector<4xf32>
      %3 = llvm.mlir.constant(63 : index) : i64
      %4 = llvm.mlir.constant(0 : index) : i64
      %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
      %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
      %9 = llvm.and %8, %3  : i64
      %10 = llvm.icmp "eq" %9, %4 : i64
      "llvm.intr.assume"(%10) : (i1) -> ()
      %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
      %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
      %16 = llvm.and %15, %3  : i64
      %17 = llvm.icmp "eq" %16, %4 : i64
      "llvm.intr.assume"(%17) : (i1) -> ()
      %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
      %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
      %23 = llvm.and %22, %3  : i64
      %24 = llvm.icmp "eq" %23, %4 : i64
      "llvm.intr.assume"(%24) : (i1) -> ()
      %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
      %26 = llvm.load %14 : !llvm.ptr -> f32
      %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
      %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
      %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
      llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
      %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
      %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
      %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
      %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
      %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
      %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
      llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
      llvm.return %1 : i32
    }
  }
}

// -----// IR Dump After LLVMCPUAssignImportOrdinals (iree-llvmcpu-assign-import-ordinals) //----- //
hal.executable.variant public @embedded_elf_arm_64 target(<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>) {
  hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
    llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>} {
      %0 = llvm.mlir.undef : vector<2xf32>
      %1 = llvm.mlir.constant(0 : i32) : i32
      %2 = llvm.mlir.undef : vector<4xf32>
      %3 = llvm.mlir.constant(63 : index) : i64
      %4 = llvm.mlir.constant(0 : index) : i64
      %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
      %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
      %9 = llvm.and %8, %3  : i64
      %10 = llvm.icmp "eq" %9, %4 : i64
      "llvm.intr.assume"(%10) : (i1) -> ()
      %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
      %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
      %16 = llvm.and %15, %3  : i64
      %17 = llvm.icmp "eq" %16, %4 : i64
      "llvm.intr.assume"(%17) : (i1) -> ()
      %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
      %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
      %23 = llvm.and %22, %3  : i64
      %24 = llvm.icmp "eq" %23, %4 : i64
      "llvm.intr.assume"(%24) : (i1) -> ()
      %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
      %26 = llvm.load %14 : !llvm.ptr -> f32
      %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
      %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
      %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
      llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
      %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
      %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
      %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
      %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
      %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
      %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
      llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
      llvm.return %1 : i32
    }
  }
}

// -----// IR Dump After LinkTargetExecutablesPass (iree-hal-link-target-executables) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After LinkExecutablesPass (iree-hal-link-executables) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %ordinal = hal.executable.export.ordinal target(@add_dispatch_0::@embedded_elf_arm_64::@add_dispatch_0_broadcast_6_f32) : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%ordinal] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveExportOrdinalsPass (iree-hal-resolve-export-ordinals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %exe = hal.executable.lookup device(%device_0 : !hal.device) executable(@add_dispatch_0) : !hal.executable
      %c0_2 = arith.constant 0 : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[%c0_2] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeResourceCachesPass (iree-hal-materialize-resource-caches) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %c-1 = arith.constant -1 : index
    %c0_0 = arith.constant 0 : index
    %0 = arith.select %value, %c0_0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %c14_i32 = arith.constant 14 : i32
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %1 = arith.select %value, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      %c0_2 = arith.constant 0 : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0_2] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After MemoizeDeviceQueriesPass (iree-hal-memoize-device-queries) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %c-1 = arith.constant -1 : index
    %c0_0 = arith.constant 0 : index
    %0 = arith.select %_device_query_0, %c0_0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %c14_i32 = arith.constant 14 : i32
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      %c0_2 = arith.constant 0 : index
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0_2] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %c-1 = arith.constant -1 : index
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      %c14_i32 = arith.constant 14 : i32
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = scf.index_switch %0 -> !hal.executable 
    case 0 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    }
    default {
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %2 = util.null : !hal.executable
      scf.yield %2 : !hal.executable
    }
    util.global.store %1, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %0 = arith.select %_device_query_0, %c0, %c-1 : index
  %1 = scf.index_switch %0 -> !hal.executable 
  case 0 {
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    scf.yield %exe : !hal.executable
  }
  default {
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    scf.yield %2 : !hal.executable
  }
  util.global.store %1, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.global.store %ok, @_device_query_0_ok : i1
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  scf.if %2 {
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  }
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.global.store %ok, @_device_query_0_ok : i1
    util.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c14_i32 = arith.constant 14 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_0 = hal.devices.get %c0 : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    %2 = scf.if %1 -> (!hal.executable) {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %3 = util.null : !hal.executable
      scf.yield %3 : !hal.executable
    }
    util.global.store %2, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c14_i32 = arith.constant 14 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_0 = hal.devices.get %c0 : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    %2 = scf.if %1 -> (!hal.executable) {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %3 = util.null : !hal.executable
      scf.yield %3 : !hal.executable
    }
    util.global.store %2, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c14_i32 = arith.constant 14 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_0 = hal.devices.get %c0 : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    %2 = scf.if %1 -> (!hal.executable) {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %3 = util.null : !hal.executable
      scf.yield %3 : !hal.executable
    }
    util.global.store %2, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.return
}

// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.return
}

// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %c14_i32 = arith.constant 14 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device_0 = hal.devices.get %c0 : !hal.device
  %0 = arith.select %_device_query_0, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  %2 = scf.if %1 -> (!hal.executable) {
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    scf.yield %exe : !hal.executable
  } else {
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %3 = util.null : !hal.executable
    scf.yield %3 : !hal.executable
  }
  util.global.store %2, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device_0 = hal.devices.get %c0 : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.return
}

// -----// IR Dump After ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c24 = arith.constant 24 : index
  %c4 = arith.constant 4 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  scf.if %2 {
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  }
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c14_i32 = arith.constant 14 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_0 = hal.devices.get %c0 : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    %2 = scf.if %1 -> (!hal.executable) {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %3 = util.null : !hal.executable
      scf.yield %3 : !hal.executable
    }
    util.global.store %2, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %c0 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0 = hal.devices.get %c0 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.return
  }
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c14_i32 = arith.constant 14 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_0 = hal.devices.get %c0 : !hal.device
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    %2 = scf.if %1 -> (!hal.executable) {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
      scf.yield %exe : !hal.executable
    } else {
      util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
      %3 = util.null : !hal.executable
      scf.yield %3 : !hal.executable
    }
    util.global.store %2, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    scf.if %2 {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
        %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
      ])
      %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device_0 = hal.devices.get %c0 : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %c14_i32 = arith.constant 14 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device_0 = hal.devices.get %c0 : !hal.device
  %0 = arith.select %_device_query_0, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  cf.br ^bb4
^bb4:  // pred: ^bb3
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c24 = arith.constant 24 : index
  %c4 = arith.constant 4 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert, {enable_loop_peeling}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %c0_0 = arith.constant 0 : index
    %device_0_1 = hal.devices.get %c0_0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0_1 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %c0_2 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0_3 = hal.devices.get %c0_2 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0_3 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %c0_4 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c14_i32 = arith.constant 14 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_0_5 = hal.devices.get %c0_4 : !hal.device
    %0 = arith.select %_device_query_0, %c0_4, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0_4 : index
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0_5 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    cf.br ^bb5
  ^bb5:  // pred: ^bb4
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.variant public @embedded_elf_arm_64 target(#executable_target_embedded_elf_arm_64_) {
      hal.executable.export public @add_dispatch_0_broadcast_6_f32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", llvm.target_triple = "aarch64-unknown-unknown-eabi-elf"} {
        llvm.func @add_dispatch_0_broadcast_6_f32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 attributes {translation_info = #translation} {
          %0 = llvm.mlir.undef : vector<2xf32>
          %1 = llvm.mlir.constant(0 : i32) : i32
          %2 = llvm.mlir.undef : vector<4xf32>
          %3 = llvm.mlir.constant(63 : index) : i64
          %4 = llvm.mlir.constant(0 : index) : i64
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i64
          %9 = llvm.and %8, %3  : i64
          %10 = llvm.icmp "eq" %9, %4 : i64
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i64
          %16 = llvm.and %15, %3  : i64
          %17 = llvm.icmp "eq" %16, %4 : i64
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %19 = llvm.extractvalue %18[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %20 = llvm.getelementptr %19[2] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %21 = llvm.load %20 : !llvm.ptr -> !llvm.ptr
          %22 = llvm.ptrtoint %21 : !llvm.ptr to i64
          %23 = llvm.and %22, %3  : i64
          %24 = llvm.icmp "eq" %23, %4 : i64
          "llvm.intr.assume"(%24) : (i1) -> ()
          %25 = llvm.load %7 {alignment = 4 : i64} : !llvm.ptr -> vector<4xf32>
          %26 = llvm.load %14 : !llvm.ptr -> f32
          %27 = llvm.insertelement %26, %2[%1 : i32] : vector<4xf32>
          %28 = llvm.shufflevector %27, %2 [0, 0, 0, 0] : vector<4xf32> 
          %29 = llvm.fadd %25, %28  {fastmathFlags = #llvm.fastmath<contract>} : vector<4xf32>
          llvm.store %29, %21 {alignment = 4 : i64} : vector<4xf32>, !llvm.ptr
          %30 = llvm.getelementptr %7[4] : (!llvm.ptr) -> !llvm.ptr, f32
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %32 = llvm.insertelement %26, %0[%1 : i32] : vector<2xf32>
          %33 = llvm.shufflevector %32, %0 [0, 0] : vector<2xf32> 
          %34 = llvm.fadd %31, %33  {fastmathFlags = #llvm.fastmath<contract>} : vector<2xf32>
          %35 = llvm.getelementptr %21[4] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %34, %35 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %1 : i32
        }
      }
    }
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SerializeTargetExecutablesPass (iree-hal-serialize-target-executables) //----- //
hal.executable private @add_dispatch_0 {
  hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"0x7F454C460201010000000000000000000300B700010000000000000000000000400000000000000050080000000000000000000040003800070040001500130006000000040000004000000000000000400000000000000040000000000000008801000000000000880100000000000008000000000000000100000004000000000000000000000000000000000000000000000000000000E803000000000000E80300000000000000000100000000000100000005000000E803000000000000E803010000000000E80301000000000050000000000000005000000000000000000001000000000001000000060000004004000000000000400402000000000040040200000000008801000000000000C00B00000000000000000100000000000200000006000000080500000000000008050200000000000805020000000000C000000000000000C000000000000000080000000000000052E57464040000004004000000000000400402000000000040040200000000008801000000000000C00B000000000000010000000000000051E574640600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000120007002404010000000000140000000000000002000000020000000000000001000000000000000000000000697265655F68616C5F65786563757461626C655F6C6962726172795F717565727900000000000048040200000000000304000000000000580300000000000058040200000000000304000000000000E803010000000000600402000000000003040000000000006C03000000000000700402000000000003040000000000008B0300000000000088040200000000000304000000000000A00300000000000090040200000000000304000000000000A003000000000000A00402000000000003040000000000004004020000000000C00402000000000003040000000000005804020000000000C80402000000000003040000000000006803000000000000D00402000000000003040000000000006004020000000000E00402000000000003040000000000006804020000000000E804020000000000030400000000000080040200000000006164645F64697370617463685F300000000000006164645F64697370617463685F305F62726F6164636173745F365F663332006164645F746F73612E6D6C697200000000000000001000000000000000017A5200017C1E011B0C1F0018000000180000002C0001003C00000000480C1D109E029D0400000010000000340000004C000100140000000000000000000000FD7BBFA9FD030091281040F9E0031F2A0A2540A9080940F920C9404D4101C03D21D4204E0101803D410940FD20D4200E000900FDFD7BC1A8C0035FD61F1000711F2003D5A803081000019F9AC0035FD6000000000000000004000000000000000000000000000000000000000000000000000000000000000000000000000000010000000D00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001E000000000000000800000000000000FBFFFF6F000000000100000000000000070000000000000038020000000000000800000000000000200100000000000009000000000000001800000000000000F9FFFF6F000000000C000000000000000600000000000000C8010000000000000B000000000000001800000000000000050000000000000010020000000000000A0000000000000023000000000000000400000000000000F80100000000000000000000000000000000000000000000011101250E1305030E1017B44219110112060000022E001101120640186E0E030E3A0B3B0B49133F190000032400030E3E0B0B0B000000470000000400000000000801250000002C002300000000000000E8030100000000003C00000002E8030100000000003C000000016D0000000000000000010143000000031F0000000504006164645F64697370617463685F305F62726F6164636173745F365F66333200696E74002D004952454500310000000200000000004B000000260000006164645F64697370617463685F305F62726F6164636173745F365F6633320000000000160000000200000000004B00000043000000696E74000000000039000000040019000000010101FB0E0D000101010100000001000001002D0000000000000902E8030100000000000105010A82060B022C1202080001014952454500000000000000000000000000000000000000000000000000000000230000000002090008050200000000000000000000000000010000001200070024040100000000001400000000000000002E64796E73796D002E68617368002E64796E737472002E72656C612E64796E002E726F64617461002E65685F6672616D65002E74657874002E646174612E72656C2E726F002E64796E616D6963002E72656C726F5F70616464696E67002E64656275675F616262726576002E64656275675F696E666F002E64656275675F737472002E64656275675F7075626E616D6573002E64656275675F7075627479706573002E64656275675F6C696E65002E636F6D6D656E74002E73796D746162002E7368737472746162002E7374727461620000697265655F68616C5F65786563757461626C655F6C6962726172795F7175657279005F44594E414D494300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000B0000000200000000000000C801000000000000C801000000000000300000000000000003000000010000000800000000000000180000000000000009000000050000000200000000000000F801000000000000F80100000000000018000000000000000100000000000000040000000000000004000000000000000F000000030000000200000000000000100200000000000010020000000000002300000000000000000000000000000001000000000000000000000000000000170000000400000002000000000000003802000000000000380200000000000020010000000000000100000000000000080000000000000018000000000000002100000001000000020000000000000058030000000000005803000000000000480000000000000000000000000000000800000000000000000000000000000029000000010000000200000000000000A003000000000000A003000000000000480000000000000000000000000000000800000000000000000000000000000033000000010000000600000000000000E803010000000000E80300000000000050000000000000000000000000000000040000000000000000000000000000003900000001000000030000000000000040040200000000004004000000000000C8000000000000000000000000000000100000000000000000000000000000004600000006000000030000000000000008050200000000000805000000000000C0000000000000000300000000000000080000000000000010000000000000004F000000080000000300000000000000C805020000000000C805000000000000380A0000000000000000000000000000010000000000000000000000000000005E0000000100000000000000000000000000000000000000C80500000000000037000000000000000000000000000000010000000000000000000000000000006C0000000100000000000000000000000000000000000000FF050000000000004B000000000000000000000000000000010000000000000000000000000000007800000001000000300000000000000000000000000000004A060000000000002A0000000000000000000000000000000100000000000000010000000000000083000000010000000000000000000000000000000000000074060000000000003500000000000000000000000000000001000000000000000000000000000000930000000100000000000000000000000000000000000000A9060000000000001A00000000000000000000000000000001000000000000000000000000000000A30000000100000000000000000000000000000000000000C3060000000000003D00000000000000000000000000000001000000000000000000000000000000AF000000010000003000000000000000000000000000000000070000000000000500000000000000000000000000000001000000000000000100000000000000B8000000020000000000000000000000000000000000000008070000000000004800000000000000140000000200000008000000000000001800000000000000C000000003000000000000000000000000000000000000005007000000000000D200000000000000000000000000000001000000000000000000000000000000CA000000030000000000000000000000000000000000000022080000000000002C00000000000000000000000000000001000000000000000000000000000000"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
}

// -----// IR Dump After SerializeExecutablesPass (iree-hal-serialize-executables) //----- //
hal.executable private @add_dispatch_0 {
  hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
}

// -----// IR Dump After PruneExecutablesPass (iree-hal-prune-executables) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %c0_0 = arith.constant 0 : index
    %device_0_1 = hal.devices.get %c0_0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0_1 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %c0_2 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0_3 = hal.devices.get %c0_2 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0_3 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %c0_4 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c14_i32 = arith.constant 14 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_0_5 = hal.devices.get %c0_4 : !hal.device
    %0 = arith.select %_device_query_0, %c0_4, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0_4 : index
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0_5 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    cf.br ^bb5
  ^bb5:  // pred: ^bb4
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %c0_0 = arith.constant 0 : index
    %device_0_1 = hal.devices.get %c0_0 : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0_1 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %c0_2 = arith.constant 0 : index
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_0_3 = hal.devices.get %c0_2 : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_0_3 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %c0_4 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c14_i32 = arith.constant 14 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_0_5 = hal.devices.get %c0_4 : !hal.device
    %0 = arith.select %_device_query_0, %c0_4, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0_4 : index
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0_5 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    cf.br ^bb5
  ^bb5:  // pred: ^bb4
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %c-1 = arith.constant -1 : index
    %c14_i32 = arith.constant 14 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    cf.br ^bb5
  ^bb5:  // pred: ^bb4
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %0 = arith.select %_device_query_0, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c24 = arith.constant 24 : index
  %c4 = arith.constant 4 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c-1 = arith.constant -1 : index
  %c-1_i32 = arith.constant -1 : i32
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"0x7F454C460201010000000000000000000300B700010000000000000000000000400000000000000050080000000000000000000040003800070040001500130006000000040000004000000000000000400000000000000040000000000000008801000000000000880100000000000008000000000000000100000004000000000000000000000000000000000000000000000000000000E803000000000000E80300000000000000000100000000000100000005000000E803000000000000E803010000000000E80301000000000050000000000000005000000000000000000001000000000001000000060000004004000000000000400402000000000040040200000000008801000000000000C00B00000000000000000100000000000200000006000000080500000000000008050200000000000805020000000000C000000000000000C000000000000000080000000000000052E57464040000004004000000000000400402000000000040040200000000008801000000000000C00B000000000000010000000000000051E574640600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000120007002404010000000000140000000000000002000000020000000000000001000000000000000000000000697265655F68616C5F65786563757461626C655F6C6962726172795F717565727900000000000048040200000000000304000000000000580300000000000058040200000000000304000000000000E803010000000000600402000000000003040000000000006C03000000000000700402000000000003040000000000008B0300000000000088040200000000000304000000000000A00300000000000090040200000000000304000000000000A003000000000000A00402000000000003040000000000004004020000000000C00402000000000003040000000000005804020000000000C80402000000000003040000000000006803000000000000D00402000000000003040000000000006004020000000000E00402000000000003040000000000006804020000000000E804020000000000030400000000000080040200000000006164645F64697370617463685F300000000000006164645F64697370617463685F305F62726F6164636173745F365F663332006164645F746F73612E6D6C697200000000000000001000000000000000017A5200017C1E011B0C1F0018000000180000002C0001003C00000000480C1D109E029D0400000010000000340000004C000100140000000000000000000000FD7BBFA9FD030091281040F9E0031F2A0A2540A9080940F920C9404D4101C03D21D4204E0101803D410940FD20D4200E000900FDFD7BC1A8C0035FD61F1000711F2003D5A803081000019F9AC0035FD6000000000000000004000000000000000000000000000000000000000000000000000000000000000000000000000000010000000D00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001E000000000000000800000000000000FBFFFF6F000000000100000000000000070000000000000038020000000000000800000000000000200100000000000009000000000000001800000000000000F9FFFF6F000000000C000000000000000600000000000000C8010000000000000B000000000000001800000000000000050000000000000010020000000000000A0000000000000023000000000000000400000000000000F80100000000000000000000000000000000000000000000011101250E1305030E1017B44219110112060000022E001101120640186E0E030E3A0B3B0B49133F190000032400030E3E0B0B0B000000470000000400000000000801250000002C002300000000000000E8030100000000003C00000002E8030100000000003C000000016D0000000000000000010143000000031F0000000504006164645F64697370617463685F305F62726F6164636173745F365F66333200696E74002D004952454500310000000200000000004B000000260000006164645F64697370617463685F305F62726F6164636173745F365F6633320000000000160000000200000000004B00000043000000696E74000000000039000000040019000000010101FB0E0D000101010100000001000001002D0000000000000902E8030100000000000105010A82060B022C1202080001014952454500000000000000000000000000000000000000000000000000000000230000000002090008050200000000000000000000000000010000001200070024040100000000001400000000000000002E64796E73796D002E68617368002E64796E737472002E72656C612E64796E002E726F64617461002E65685F6672616D65002E74657874002E646174612E72656C2E726F002E64796E616D6963002E72656C726F5F70616464696E67002E64656275675F616262726576002E64656275675F696E666F002E64656275675F737472002E64656275675F7075626E616D6573002E64656275675F7075627479706573002E64656275675F6C696E65002E636F6D6D656E74002E73796D746162002E7368737472746162002E7374727461620000697265655F68616C5F65786563757461626C655F6C6962726172795F7175657279005F44594E414D494300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000B0000000200000000000000C801000000000000C801000000000000300000000000000003000000010000000800000000000000180000000000000009000000050000000200000000000000F801000000000000F80100000000000018000000000000000100000000000000040000000000000004000000000000000F000000030000000200000000000000100200000000000010020000000000002300000000000000000000000000000001000000000000000000000000000000170000000400000002000000000000003802000000000000380200000000000020010000000000000100000000000000080000000000000018000000000000002100000001000000020000000000000058030000000000005803000000000000480000000000000000000000000000000800000000000000000000000000000029000000010000000200000000000000A003000000000000A003000000000000480000000000000000000000000000000800000000000000000000000000000033000000010000000600000000000000E803010000000000E80300000000000050000000000000000000000000000000040000000000000000000000000000003900000001000000030000000000000040040200000000004004000000000000C8000000000000000000000000000000100000000000000000000000000000004600000006000000030000000000000008050200000000000805000000000000C0000000000000000300000000000000080000000000000010000000000000004F000000080000000300000000000000C805020000000000C805000000000000380A0000000000000000000000000000010000000000000000000000000000005E0000000100000000000000000000000000000000000000C80500000000000037000000000000000000000000000000010000000000000000000000000000006C0000000100000000000000000000000000000000000000FF050000000000004B000000000000000000000000000000010000000000000000000000000000007800000001000000300000000000000000000000000000004A060000000000002A0000000000000000000000000000000100000000000000010000000000000083000000010000000000000000000000000000000000000074060000000000003500000000000000000000000000000001000000000000000000000000000000930000000100000000000000000000000000000000000000A9060000000000001A00000000000000000000000000000001000000000000000000000000000000A30000000100000000000000000000000000000000000000C3060000000000003D00000000000000000000000000000001000000000000000000000000000000AF000000010000003000000000000000000000000000000000070000000000000500000000000000000000000000000001000000000000000100000000000000B8000000020000000000000000000000000000000000000008070000000000004800000000000000140000000200000008000000000000001800000000000000C000000003000000000000000000000000000000000000005007000000000000D200000000000000000000000000000001000000000000000000000000000000CA000000030000000000000000000000000000000000000022080000000000002C00000000000000000000000000000001000000000000000000000000000000"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
util.initializer {
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
util.initializer {
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
util.initializer {
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
util.initializer {
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
util.initializer {
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c-1_i32 = arith.constant -1 : i32
    %c-1 = arith.constant -1 : index
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c4 = arith.constant 4 : index
    %c24 = arith.constant 24 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %c14_i32 = arith.constant 14 : i32
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %device_0 = hal.devices.get %c0 : !hal.device
  %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  %0 = arith.select %value, %c0, %c-1 : index
  %1 = arith.cmpi eq, %0, %c0 : index
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %1, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
  %2 = util.null : !hal.executable
  cf.br ^bb3(%2 : !hal.executable)
^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
  util.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
  %c-1_i32 = arith.constant -1 : i32
  %c-1 = arith.constant -1 : index
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4 = arith.constant 4 : index
  %c24 = arith.constant 24 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %element_type_f32 = hal.element_type<f32> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device_0 = hal.devices.get %c0 : !hal.device
  %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
  %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %1 = arith.select %_device_query_0, %c0, %c-1 : index
  %2 = arith.cmpi eq, %1, %c0 : index
  cf.cond_br %2, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
    %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
  cf.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
  util.return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"0x7F454C460201010000000000000000000300B700010000000000000000000000400000000000000050080000000000000000000040003800070040001500130006000000040000004000000000000000400000000000000040000000000000008801000000000000880100000000000008000000000000000100000004000000000000000000000000000000000000000000000000000000E803000000000000E80300000000000000000100000000000100000005000000E803000000000000E803010000000000E80301000000000050000000000000005000000000000000000001000000000001000000060000004004000000000000400402000000000040040200000000008801000000000000C00B00000000000000000100000000000200000006000000080500000000000008050200000000000805020000000000C000000000000000C000000000000000080000000000000052E57464040000004004000000000000400402000000000040040200000000008801000000000000C00B000000000000010000000000000051E574640600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000120007002404010000000000140000000000000002000000020000000000000001000000000000000000000000697265655F68616C5F65786563757461626C655F6C6962726172795F717565727900000000000048040200000000000304000000000000580300000000000058040200000000000304000000000000E803010000000000600402000000000003040000000000006C03000000000000700402000000000003040000000000008B0300000000000088040200000000000304000000000000A00300000000000090040200000000000304000000000000A003000000000000A00402000000000003040000000000004004020000000000C00402000000000003040000000000005804020000000000C80402000000000003040000000000006803000000000000D00402000000000003040000000000006004020000000000E00402000000000003040000000000006804020000000000E804020000000000030400000000000080040200000000006164645F64697370617463685F300000000000006164645F64697370617463685F305F62726F6164636173745F365F663332006164645F746F73612E6D6C697200000000000000001000000000000000017A5200017C1E011B0C1F0018000000180000002C0001003C00000000480C1D109E029D0400000010000000340000004C000100140000000000000000000000FD7BBFA9FD030091281040F9E0031F2A0A2540A9080940F920C9404D4101C03D21D4204E0101803D410940FD20D4200E000900FDFD7BC1A8C0035FD61F1000711F2003D5A803081000019F9AC0035FD6000000000000000004000000000000000000000000000000000000000000000000000000000000000000000000000000010000000D00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001E000000000000000800000000000000FBFFFF6F000000000100000000000000070000000000000038020000000000000800000000000000200100000000000009000000000000001800000000000000F9FFFF6F000000000C000000000000000600000000000000C8010000000000000B000000000000001800000000000000050000000000000010020000000000000A0000000000000023000000000000000400000000000000F80100000000000000000000000000000000000000000000011101250E1305030E1017B44219110112060000022E001101120640186E0E030E3A0B3B0B49133F190000032400030E3E0B0B0B000000470000000400000000000801250000002C002300000000000000E8030100000000003C00000002E8030100000000003C000000016D0000000000000000010143000000031F0000000504006164645F64697370617463685F305F62726F6164636173745F365F66333200696E74002D004952454500310000000200000000004B000000260000006164645F64697370617463685F305F62726F6164636173745F365F6633320000000000160000000200000000004B00000043000000696E74000000000039000000040019000000010101FB0E0D000101010100000001000001002D0000000000000902E8030100000000000105010A82060B022C1202080001014952454500000000000000000000000000000000000000000000000000000000230000000002090008050200000000000000000000000000010000001200070024040100000000001400000000000000002E64796E73796D002E68617368002E64796E737472002E72656C612E64796E002E726F64617461002E65685F6672616D65002E74657874002E646174612E72656C2E726F002E64796E616D6963002E72656C726F5F70616464696E67002E64656275675F616262726576002E64656275675F696E666F002E64656275675F737472002E64656275675F7075626E616D6573002E64656275675F7075627479706573002E64656275675F6C696E65002E636F6D6D656E74002E73796D746162002E7368737472746162002E7374727461620000697265655F68616C5F65786563757461626C655F6C6962726172795F7175657279005F44594E414D494300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000B0000000200000000000000C801000000000000C801000000000000300000000000000003000000010000000800000000000000180000000000000009000000050000000200000000000000F801000000000000F80100000000000018000000000000000100000000000000040000000000000004000000000000000F000000030000000200000000000000100200000000000010020000000000002300000000000000000000000000000001000000000000000000000000000000170000000400000002000000000000003802000000000000380200000000000020010000000000000100000000000000080000000000000018000000000000002100000001000000020000000000000058030000000000005803000000000000480000000000000000000000000000000800000000000000000000000000000029000000010000000200000000000000A003000000000000A003000000000000480000000000000000000000000000000800000000000000000000000000000033000000010000000600000000000000E803010000000000E80300000000000050000000000000000000000000000000040000000000000000000000000000003900000001000000030000000000000040040200000000004004000000000000C8000000000000000000000000000000100000000000000000000000000000004600000006000000030000000000000008050200000000000805000000000000C0000000000000000300000000000000080000000000000010000000000000004F000000080000000300000000000000C805020000000000C805000000000000380A0000000000000000000000000000010000000000000000000000000000005E0000000100000000000000000000000000000000000000C80500000000000037000000000000000000000000000000010000000000000000000000000000006C0000000100000000000000000000000000000000000000FF050000000000004B000000000000000000000000000000010000000000000000000000000000007800000001000000300000000000000000000000000000004A060000000000002A0000000000000000000000000000000100000000000000010000000000000083000000010000000000000000000000000000000000000074060000000000003500000000000000000000000000000001000000000000000000000000000000930000000100000000000000000000000000000000000000A9060000000000001A00000000000000000000000000000001000000000000000000000000000000A30000000100000000000000000000000000000000000000C3060000000000003D00000000000000000000000000000001000000000000000000000000000000AF000000010000003000000000000000000000000000000000070000000000000500000000000000000000000000000001000000000000000100000000000000B8000000020000000000000000000000000000000000000008070000000000004800000000000000140000000200000008000000000000001800000000000000C000000003000000000000000000000000000000000000005007000000000000D200000000000000000000000000000001000000000000000000000000000000CA000000030000000000000000000000000000000000000022080000000000002C00000000000000000000000000000001000000000000000000000000000000"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_add_dispatch_0 : !hal.executable
  util.initializer {
    %c14_i32 = arith.constant 14 : i32
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %device_0 = hal.devices.get %c0 : !hal.device
    %ok, %value = hal.device.query<%device_0 : !hal.device> key("hal.executable.format" :: "embedded-elf-arm_64") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device_0 : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    %0 = arith.select %value, %c0, %c-1 : index
    %1 = arith.cmpi eq, %0, %c0 : index
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %1, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_0 : !hal.device) target(@add_dispatch_0::@embedded_elf_arm_64) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    util.status.check_ok %c14_i32, "none of the executable binaries in the module are supported by the runtime"
    %2 = util.null : !hal.executable
    cf.br ^bb3(%2 : !hal.executable)
  ^bb3(%3: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %3, @_executable_add_dispatch_0 : !hal.executable
    util.return
  }
  hal.executable private @add_dispatch_0 {
    hal.executable.binary public @embedded_elf_arm_64 attributes {data = dense<"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"> : vector<3472xi8>, format = "embedded-elf-arm_64", mime_type = "application/x-elf"}
  }
  util.func public @add(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %c24 = arith.constant 24 : index
    %c4 = arith.constant 4 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1 = arith.constant -1 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_add_dispatch_0 = util.global.load @_executable_add_dispatch_0 : !hal.executable
    %element_type_f32 = hal.element_type<f32> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device_0 = hal.devices.get %c0 : !hal.device
    %allocator = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c24) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c1, %c1]) type(%element_type_f32) encoding(%dense_row_major)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c4) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device_0 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c24}
    %cmd = hal.command_buffer.create device(%device_0 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = arith.select %_device_query_0, %c0, %c-1 : index
    %2 = arith.cmpi eq, %1, %c0 : index
    cf.cond_br %2, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c24], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c4], 
      %c2 = (%transient_buffer : !hal.buffer)[%c0, %c24]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_add_dispatch_0 : !hal.executable)[%c0] workgroups([%c1, %c1, %c1])
    cf.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device_0 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_0 : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %status = hal.fence.await until([%fence_1]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%transient_buffer : !hal.buffer)[%c0, %c24] shape([%c3, %c2]) type(%element_type_f32) encoding(%dense_row_major) : !hal.buffer_view
    util.return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ConversionPass (iree-vm-conversion) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"0x7F454C460201010000000000000000000300B700010000000000000000000000400000000000000050080000000000000000000040003800070040001500130006000000040000004000000000000000400000000000000040000000000000008801000000000000880100000000000008000000000000000100000004000000000000000000000000000000000000000000000000000000E803000000000000E80300000000000000000100000000000100000005000000E803000000000000E803010000000000E80301000000000050000000000000005000000000000000000001000000000001000000060000004004000000000000400402000000000040040200000000008801000000000000C00B00000000000000000100000000000200000006000000080500000000000008050200000000000805020000000000C000000000000000C000000000000000080000000000000052E57464040000004004000000000000400402000000000040040200000000008801000000000000C00B000000000000010000000000000051E574640600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000120007002404010000000000140000000000000002000000020000000000000001000000000000000000000000697265655F68616C5F65786563757461626C655F6C6962726172795F717565727900000000000048040200000000000304000000000000580300000000000058040200000000000304000000000000E803010000000000600402000000000003040000000000006C03000000000000700402000000000003040000000000008B0300000000000088040200000000000304000000000000A00300000000000090040200000000000304000000000000A003000000000000A00402000000000003040000000000004004020000000000C00402000000000003040000000000005804020000000000C80402000000000003040000000000006803000000000000D00402000000000003040000000000006004020000000000E00402000000000003040000000000006804020000000000E804020000000000030400000000000080040200000000006164645F64697370617463685F300000000000006164645F64697370617463685F305F62726F6164636173745F365F663332006164645F746F73612E6D6C697200000000000000001000000000000000017A5200017C1E011B0C1F0018000000180000002C0001003C00000000480C1D109E029D0400000010000000340000004C000100140000000000000000000000FD7BBFA9FD030091281040F9E0031F2A0A2540A9080940F920C9404D4101C03D21D4204E0101803D410940FD20D4200E000900FDFD7BC1A8C0035FD61F1000711F2003D5A803081000019F9AC0035FD6000000000000000004000000000000000000000000000000000000000000000000000000000000000000000000000000010000000D00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001E000000000000000800000000000000FBFFFF6F000000000100000000000000070000000000000038020000000000000800000000000000200100000000000009000000000000001800000000000000F9FFFF6F000000000C000000000000000600000000000000C8010000000000000B000000000000001800000000000000050000000000000010020000000000000A0000000000000023000000000000000400000000000000F80100000000000000000000000000000000000000000000011101250E1305030E1017B44219110112060000022E001101120640186E0E030E3A0B3B0B49133F190000032400030E3E0B0B0B000000470000000400000000000801250000002C002300000000000000E8030100000000003C00000002E8030100000000003C000000016D0000000000000000010143000000031F0000000504006164645F64697370617463685F305F62726F6164636173745F365F66333200696E74002D004952454500310000000200000000004B000000260000006164645F64697370617463685F305F62726F6164636173745F365F6633320000000000160000000200000000004B00000043000000696E74000000000039000000040019000000010101FB0E0D000101010100000001000001002D0000000000000902E8030100000000000105010A82060B022C1202080001014952454500000000000000000000000000000000000000000000000000000000230000000002090008050200000000000000000000000000010000001200070024040100000000001400000000000000002E64796E73796D002E68617368002E64796E737472002E72656C612E64796E002E726F64617461002E65685F6672616D65002E74657874002E646174612E72656C2E726F002E64796E616D6963002E72656C726F5F70616464696E67002E64656275675F616262726576002E64656275675F696E666F002E64656275675F737472002E64656275675F7075626E616D6573002E64656275675F7075627479706573002E64656275675F6C696E65002E636F6D6D656E74002E73796D746162002E7368737472746162002E7374727461620000697265655F68616C5F65786563757461626C655F6C6962726172795F7175657279005F44594E414D494300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000B0000000200000000000000C801000000000000C801000000000000300000000000000003000000010000000800000000000000180000000000000009000000050000000200000000000000F801000000000000F80100000000000018000000000000000100000000000000040000000000000004000000000000000F000000030000000200000000000000100200000000000010020000000000002300000000000000000000000000000001000000000000000000000000000000170000000400000002000000000000003802000000000000380200000000000020010000000000000100000000000000080000000000000018000000000000002100000001000000020000000000000058030000000000005803000000000000480000000000000000000000000000000800000000000000000000000000000029000000010000000200000000000000A003000000000000A003000000000000480000000000000000000000000000000800000000000000000000000000000033000000010000000600000000000000E803010000000000E80300000000000050000000000000000000000000000000040000000000000000000000000000003900000001000000030000000000000040040200000000004004000000000000C8000000000000000000000000000000100000000000000000000000000000004600000006000000030000000000000008050200000000000805000000000000C0000000000000000300000000000000080000000000000010000000000000004F000000080000000300000000000000C805020000000000C805000000000000380A0000000000000000000000000000010000000000000000000000000000005E0000000100000000000000000000000000000000000000C80500000000000037000000000000000000000000000000010000000000000000000000000000006C0000000100000000000000000000000000000000000000FF050000000000004B000000000000000000000000000000010000000000000000000000000000007800000001000000300000000000000000000000000000004A060000000000002A0000000000000000000000000000000100000000000000010000000000000083000000010000000000000000000000000000000000000074060000000000003500000000000000000000000000000001000000000000000000000000000000930000000100000000000000000000000000000000000000A9060000000000001A00000000000000000000000000000001000000000000000000000000000000A30000000100000000000000000000000000000000000000C3060000000000003D00000000000000000000000000000001000000000000000000000000000000AF000000010000003000000000000000000000000000000000070000000000000500000000000000000000000000000001000000000000000100000000000000B8000000020000000000000000000000000000000000000008070000000000004800000000000000140000000200000008000000000000001800000000000000C000000003000000000000000000000000000000000000005007000000000000D200000000000000000000000000000001000000000000000000000000000000CA000000030000000000000000000000000000000000000022080000000000002C00000000000000000000000000000001000000000000000000000000000000"> : vector<3472xi8>
    vm.initializer {
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %buffer = vm.rodata.inline "_utf8_hal_executable_format_EAB228F999C2D3A1" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
      %buffer_1 = vm.rodata.inline "_utf8_embedded_elf_arm_64_48330F48BB3EF6E0" {alignment = 1 : i64} : !vm.buffer = "embedded-elf-arm_64"
      %0:2 = vm.call @hal.device.query.i64(%ref, %buffer, %buffer_1) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %zero_2 = vm.const.i32.zero
      %1 = vm.select.i32 %0#0, %nz, %zero_2 : i32
      %c1 = vm.const.i32 1
      %zero_3 = vm.const.i32.zero
      %zero_4 = vm.const.i32.zero
      %c7 = vm.const.i32 7
      %c1_5 = vm.const.i32 1
      %c1_6 = vm.const.i32 1
      %c7_7 = vm.const.i32 7
      %c1_8 = vm.const.i32 1
      %c2 = vm.const.i32 2
      %c7_9 = vm.const.i32 7
      %zero_10 = vm.const.i32.zero
      %ref_11 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_3, [(%zero_4, %c7, %c1_5), (%c1_6, %c7_7, %c1_8), (%c2, %c7_9, %zero_10)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %zero_12 = vm.const.i32.zero
      %ref_13 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_12, [%ref_11]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_13, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %buffer_14 = vm.rodata.inline "_utf8_embedded_elf_arm_64_48330F48BB3EF6E0" {alignment = 1 : i64} : !vm.buffer = "embedded-elf-arm_64"
      %null = vm.const.ref.zero : !vm.buffer
      %ref_15 = vm.call.variadic @hal.executable.create(%ref, %buffer_14, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_15 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.cond_fail %c14, "none of the executable binaries in the module are supported by the runtime"
      %null_16 = vm.const.ref.zero : !vm.ref<!hal.executable>
      vm.br ^bb3(%null_16 : !vm.ref<!hal.executable>)
    ^bb3(%3: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c3 = vm.const.i64 3
      %c2 = vm.const.i64 2
      %c1 = vm.const.i64 1
      %zero = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %zero_0 = vm.const.i64.zero
      %c-1_1 = vm.const.i64 -1
      %c-1_2 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %c553648160 = vm.const.i32 553648160
      %c1_3 = vm.const.i32 1
      %buffer = vm.rodata.inline "_utf8_input0_F74E046E5FFA3735" {alignment = 1 : i64} : !vm.buffer = "input0"
      vm.call.variadic @hal.buffer_view.assert(%arg0, %buffer, %c553648160, %c1_3, [%c3, %c2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %zero_4 = vm.const.i32.zero
      %ref_5 = vm.call @hal.devices.get(%zero_4) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %buffer_7 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16 = vm.const.i32 16
      %c3075 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref, %buffer_7, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %buffer_8 = vm.rodata.inline "_utf8_input1_E2E5222C371315B9" {alignment = 1 : i64} : !vm.buffer = "input1"
      vm.call.variadic @hal.buffer_view.assert(%arg1, %buffer_8, %c553648160, %c1_3, [%c1, %c1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %buffer_10 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16_11 = vm.const.i32 16
      %c3075_12 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref_9, %buffer_10, %ref_6, %c4, %c16_11, %c3075_12) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %zero_13 = vm.const.i32.zero
      %ref_14 = vm.call @hal.fence.create(%ref_5, %zero_13) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %zero_15 = vm.const.i32.zero
      %c48 = vm.const.i32 48
      %c3075_16 = vm.const.i32 3075
      %ref_17 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_14, %zero_15, %c48, %c3075_16, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %c1_18 = vm.const.i32 1
      %c3_19 = vm.const.i32 3
      %zero_20 = vm.const.i32.zero
      %ref_21 = vm.call @hal.command_buffer.create(%ref_5, %c1_18, %c3_19, %zero_20) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero, %c-1_1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %zero_22 = vm.const.i32.zero
      %zero_23 = vm.const.i32.zero
      %zero_24 = vm.const.i32.zero
      %c1_25 = vm.const.i32 1
      %c2_26 = vm.const.i32 2
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_21, %_pipeline_layout_0, %zero_22, [(%zero_23, %zero_24, %ref, %zero, %c24), (%c1_25, %zero_24, %ref_9, %zero, %c4), (%c2_26, %zero_24, %ref_17, %zero, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      %zero_27 = vm.const.i32.zero
      %c1_28 = vm.const.i32 1
      %c1_29 = vm.const.i32 1
      %c1_30 = vm.const.i32 1
      vm.call @hal.command_buffer.dispatch(%ref_21, %_executable_add_dispatch_0, %zero_27, %c1_28, %c1_29, %c1_30) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      %c28 = vm.const.i32 28
      %c13 = vm.const.i32 13
      %zero_31 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref_21, %c28, %c13, %zero_31) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_21) : (!vm.ref<!hal.command_buffer>) -> ()
      %zero_32 = vm.const.i32.zero
      %ref_33 = vm.call @hal.fence.create(%ref_5, %zero_32) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_14, %ref_33, [%ref_21]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_2, [%ref_33]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_fail %1, "failed to wait on timepoint"
      %ref_34 = vm.call.variadic @hal.buffer_view.create(%ref_17, %zero, %c24, %c553648160, %c1_3, [%c3, %c2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_34 : !vm.ref<!hal.buffer_view>
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ReifyRodataTablesPass (iree-vm-reify-rodata-tables) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.initializer {
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %buffer = vm.rodata.inline "_utf8_hal_executable_format_EAB228F999C2D3A1" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
    %buffer_1 = vm.rodata.inline "_utf8_embedded_elf_arm_64_48330F48BB3EF6E0" {alignment = 1 : i64} : !vm.buffer = "embedded-elf-arm_64"
    %0:2 = vm.call @hal.device.query.i64(%ref, %buffer, %buffer_1) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %zero_2 = vm.const.i32.zero
    %1 = vm.select.i32 %0#0, %nz, %zero_2 : i32
    %c1 = vm.const.i32 1
    %zero_3 = vm.const.i32.zero
    %zero_4 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_5 = vm.const.i32 1
    %c1_6 = vm.const.i32 1
    %c7_7 = vm.const.i32 7
    %c1_8 = vm.const.i32 1
    %c2 = vm.const.i32 2
    %c7_9 = vm.const.i32 7
    %zero_10 = vm.const.i32.zero
    %ref_11 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_3, [(%zero_4, %c7, %c1_5), (%c1_6, %c7_7, %c1_8), (%c2, %c7_9, %zero_10)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_12 = vm.const.i32.zero
    %ref_13 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_12, [%ref_11]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_13, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %buffer_14 = vm.rodata.inline "_utf8_embedded_elf_arm_64_48330F48BB3EF6E0" {alignment = 1 : i64} : !vm.buffer = "embedded-elf-arm_64"
    %null = vm.const.ref.zero : !vm.buffer
    %ref_15 = vm.call.variadic @hal.executable.create(%ref, %buffer_14, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_15 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.cond_fail %c14, "none of the executable binaries in the module are supported by the runtime"
    %null_16 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_16 : !vm.ref<!hal.executable>)
  ^bb3(%3: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = vm.const.i64 3
    %c2 = vm.const.i64 2
    %c1 = vm.const.i64 1
    %zero = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %zero_0 = vm.const.i64.zero
    %c-1_1 = vm.const.i64 -1
    %c-1_2 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %c553648160 = vm.const.i32 553648160
    %c1_3 = vm.const.i32 1
    %buffer = vm.rodata.inline "_utf8_input0_F74E046E5FFA3735" {alignment = 1 : i64} : !vm.buffer = "input0"
    vm.call.variadic @hal.buffer_view.assert(%arg0, %buffer, %c553648160, %c1_3, [%c3, %c2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %zero_4 = vm.const.i32.zero
    %ref_5 = vm.call @hal.devices.get(%zero_4) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %buffer_7 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %buffer_7, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %buffer_8 = vm.rodata.inline "_utf8_input1_E2E5222C371315B9" {alignment = 1 : i64} : !vm.buffer = "input1"
    vm.call.variadic @hal.buffer_view.assert(%arg1, %buffer_8, %c553648160, %c1_3, [%c1, %c1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %buffer_10 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
    %c16_11 = vm.const.i32 16
    %c3075_12 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_9, %buffer_10, %ref_6, %c4, %c16_11, %c3075_12) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_13 = vm.const.i32.zero
    %ref_14 = vm.call @hal.fence.create(%ref_5, %zero_13) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_15 = vm.const.i32.zero
    %c48 = vm.const.i32 48
    %c3075_16 = vm.const.i32 3075
    %ref_17 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_14, %zero_15, %c48, %c3075_16, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %c1_18 = vm.const.i32 1
    %c3_19 = vm.const.i32 3
    %zero_20 = vm.const.i32.zero
    %ref_21 = vm.call @hal.command_buffer.create(%ref_5, %c1_18, %c3_19, %zero_20) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero, %c-1_1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %zero_22 = vm.const.i32.zero
    %zero_23 = vm.const.i32.zero
    %zero_24 = vm.const.i32.zero
    %c1_25 = vm.const.i32 1
    %c2_26 = vm.const.i32 2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_21, %_pipeline_layout_0, %zero_22, [(%zero_23, %zero_24, %ref, %zero, %c24), (%c1_25, %zero_24, %ref_9, %zero, %c4), (%c2_26, %zero_24, %ref_17, %zero, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_27 = vm.const.i32.zero
    %c1_28 = vm.const.i32 1
    %c1_29 = vm.const.i32 1
    %c1_30 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_21, %_executable_add_dispatch_0, %zero_27, %c1_28, %c1_29, %c1_30) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_31 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_21, %c28, %c13, %zero_31) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_21) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_32 = vm.const.i32.zero
    %ref_33 = vm.call @hal.fence.create(%ref_5, %zero_32) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_14, %ref_33, [%ref_21]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_2, [%ref_33]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %1, "failed to wait on timepoint"
    %ref_34 = vm.call.variadic @hal.buffer_view.create(%ref_17, %zero, %c24, %c553648160, %c1_3, [%c3, %c2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_34 : !vm.ref<!hal.buffer_view>
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::HoistInlinedRodataPass (iree-vm-hoist-inlined-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0_0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.initializer {
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %zero_1 = vm.const.i32.zero
    %1 = vm.select.i32 %0#0, %nz, %zero_1 : i32
    %c1 = vm.const.i32 1
    %zero_2 = vm.const.i32.zero
    %zero_3 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_4 = vm.const.i32 1
    %c1_5 = vm.const.i32 1
    %c7_6 = vm.const.i32 7
    %c1_7 = vm.const.i32 1
    %c2 = vm.const.i32 2
    %c7_8 = vm.const.i32 7
    %zero_9 = vm.const.i32.zero
    %ref_10 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_2, [(%zero_3, %c7, %c1_4), (%c1_5, %c7_6, %c1_7), (%c2, %c7_8, %zero_9)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_11 = vm.const.i32.zero
    %ref_12 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_11, [%ref_10]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_12, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0_0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0_0 : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_13 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0_0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_13 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.cond_fail %c14, "none of the executable binaries in the module are supported by the runtime"
    %null_14 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_14 : !vm.ref<!hal.executable>)
  ^bb3(%3: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC_1 {alignment = 1 : i64} "tensor"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = vm.const.i64 3
    %c2 = vm.const.i64 2
    %c1 = vm.const.i64 1
    %zero = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %zero_0 = vm.const.i64.zero
    %c-1_1 = vm.const.i64 -1
    %c-1_2 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %c553648160 = vm.const.i32 553648160
    %c1_3 = vm.const.i32 1
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1_3, [%c3, %c2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %zero_4 = vm.const.i32.zero
    %ref_5 = vm.call @hal.devices.get(%zero_4) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1_3, [%c1, %c1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_3C6209B4FD120BDC_1 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC_1 : !vm.buffer
    %c16_8 = vm.const.i32 16
    %c3075_9 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC_1, %ref_6, %c4, %c16_8, %c3075_9) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_10 = vm.const.i32.zero
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero_10) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_12 = vm.const.i32.zero
    %c48 = vm.const.i32 48
    %c3075_13 = vm.const.i32 3075
    %ref_14 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_11, %zero_12, %c48, %c3075_13, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %c1_15 = vm.const.i32 1
    %c3_16 = vm.const.i32 3
    %zero_17 = vm.const.i32.zero
    %ref_18 = vm.call @hal.command_buffer.create(%ref_5, %c1_15, %c3_16, %zero_17) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero, %c-1_1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %zero_19 = vm.const.i32.zero
    %zero_20 = vm.const.i32.zero
    %zero_21 = vm.const.i32.zero
    %c1_22 = vm.const.i32 1
    %c2_23 = vm.const.i32 2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_18, %_pipeline_layout_0, %zero_19, [(%zero_20, %zero_21, %ref, %zero, %c24), (%c1_22, %zero_21, %ref_7, %zero, %c4), (%c2_23, %zero_21, %ref_14, %zero, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_24 = vm.const.i32.zero
    %c1_25 = vm.const.i32 1
    %c1_26 = vm.const.i32 1
    %c1_27 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_18, %_executable_add_dispatch_0, %zero_24, %c1_25, %c1_26, %c1_27) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_28 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_18, %c28, %c13, %zero_28) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_18) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_29 = vm.const.i32.zero
    %ref_30 = vm.call @hal.fence.create(%ref_5, %zero_29) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_11, %ref_30, [%ref_18]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_2, [%ref_30]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %1, "failed to wait on timepoint"
    %ref_31 = vm.call.variadic @hal.buffer_view.create(%ref_14, %zero, %c24, %c553648160, %c1_3, [%c3, %c2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_31 : !vm.ref<!hal.buffer_view>
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DeduplicateRodataPass (iree-vm-deduplicate-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.initializer {
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %zero_1 = vm.const.i32.zero
    %1 = vm.select.i32 %0#0, %nz, %zero_1 : i32
    %c1 = vm.const.i32 1
    %zero_2 = vm.const.i32.zero
    %zero_3 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_4 = vm.const.i32 1
    %c1_5 = vm.const.i32 1
    %c7_6 = vm.const.i32 7
    %c1_7 = vm.const.i32 1
    %c2 = vm.const.i32 2
    %c7_8 = vm.const.i32 7
    %zero_9 = vm.const.i32.zero
    %ref_10 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_2, [(%zero_3, %c7, %c1_4), (%c1_5, %c7_6, %c1_7), (%c2, %c7_8, %zero_9)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_11 = vm.const.i32.zero
    %ref_12 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_11, [%ref_10]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_12, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0_13 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_14 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0_13, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_14 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.cond_fail %c14, "none of the executable binaries in the module are supported by the runtime"
    %null_15 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_15 : !vm.ref<!hal.executable>)
  ^bb3(%3: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c3 = vm.const.i64 3
    %c2 = vm.const.i64 2
    %c1 = vm.const.i64 1
    %zero = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %zero_0 = vm.const.i64.zero
    %c-1_1 = vm.const.i64 -1
    %c-1_2 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %c553648160 = vm.const.i32 553648160
    %c1_3 = vm.const.i32 1
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1_3, [%c3, %c2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %zero_4 = vm.const.i32.zero
    %ref_5 = vm.call @hal.devices.get(%zero_4) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1_3, [%c1, %c1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_3C6209B4FD120BDC_8 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16_9 = vm.const.i32 16
    %c3075_10 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC_8, %ref_6, %c4, %c16_9, %c3075_10) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_11 = vm.const.i32.zero
    %ref_12 = vm.call @hal.fence.create(%ref_5, %zero_11) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_13 = vm.const.i32.zero
    %c48 = vm.const.i32 48
    %c3075_14 = vm.const.i32 3075
    %ref_15 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_12, %zero_13, %c48, %c3075_14, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %c1_16 = vm.const.i32 1
    %c3_17 = vm.const.i32 3
    %zero_18 = vm.const.i32.zero
    %ref_19 = vm.call @hal.command_buffer.create(%ref_5, %c1_16, %c3_17, %zero_18) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero, %c-1_1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %zero_20 = vm.const.i32.zero
    %zero_21 = vm.const.i32.zero
    %zero_22 = vm.const.i32.zero
    %c1_23 = vm.const.i32 1
    %c2_24 = vm.const.i32 2
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_19, %_pipeline_layout_0, %zero_20, [(%zero_21, %zero_22, %ref, %zero, %c24), (%c1_23, %zero_22, %ref_7, %zero, %c4), (%c2_24, %zero_22, %ref_15, %zero, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_25 = vm.const.i32.zero
    %c1_26 = vm.const.i32 1
    %c1_27 = vm.const.i32 1
    %c1_28 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_19, %_executable_add_dispatch_0, %zero_25, %c1_26, %c1_27, %c1_28) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_29 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_19, %c28, %c13, %zero_29) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_19) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_30 = vm.const.i32.zero
    %ref_31 = vm.call @hal.fence.create(%ref_5, %zero_30) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_12, %ref_31, [%ref_19]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_2, [%ref_31]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %1, "failed to wait on timepoint"
    %ref_32 = vm.call.variadic @hal.buffer_view.create(%ref_15, %zero, %c24, %c553648160, %c1_3, [%c3, %c2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_32 : !vm.ref<!hal.buffer_view>
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0_3 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0_3, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_4, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_3C6209B4FD120BDC_8 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC_8, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_10 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_9, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_10, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_9, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4(%1 : i32), ^bb3
    ^bb3:  // pred: ^bb2
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb4(%2: i32):  // pred: ^bb2
      vm.fail %2, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4(%1 : i32), ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4(%2: i32):  // pred: ^bb2
      vm.fail %2, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"0x7F454C460201010000000000000000000300B700010000000000000000000000400000000000000050080000000000000000000040003800070040001500130006000000040000004000000000000000400000000000000040000000000000008801000000000000880100000000000008000000000000000100000004000000000000000000000000000000000000000000000000000000E803000000000000E80300000000000000000100000000000100000005000000E803000000000000E803010000000000E80301000000000050000000000000005000000000000000000001000000000001000000060000004004000000000000400402000000000040040200000000008801000000000000C00B00000000000000000100000000000200000006000000080500000000000008050200000000000805020000000000C000000000000000C000000000000000080000000000000052E57464040000004004000000000000400402000000000040040200000000008801000000000000C00B000000000000010000000000000051E574640600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000120007002404010000000000140000000000000002000000020000000000000001000000000000000000000000697265655F68616C5F65786563757461626C655F6C6962726172795F717565727900000000000048040200000000000304000000000000580300000000000058040200000000000304000000000000E803010000000000600402000000000003040000000000006C03000000000000700402000000000003040000000000008B0300000000000088040200000000000304000000000000A00300000000000090040200000000000304000000000000A003000000000000A00402000000000003040000000000004004020000000000C00402000000000003040000000000005804020000000000C80402000000000003040000000000006803000000000000D00402000000000003040000000000006004020000000000E00402000000000003040000000000006804020000000000E804020000000000030400000000000080040200000000006164645F64697370617463685F300000000000006164645F64697370617463685F305F62726F6164636173745F365F663332006164645F746F73612E6D6C697200000000000000001000000000000000017A5200017C1E011B0C1F0018000000180000002C0001003C00000000480C1D109E029D0400000010000000340000004C000100140000000000000000000000FD7BBFA9FD030091281040F9E0031F2A0A2540A9080940F920C9404D4101C03D21D4204E0101803D410940FD20D4200E000900FDFD7BC1A8C0035FD61F1000711F2003D5A803081000019F9AC0035FD6000000000000000004000000000000000000000000000000000000000000000000000000000000000000000000000000010000000D00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001E000000000000000800000000000000FBFFFF6F000000000100000000000000070000000000000038020000000000000800000000000000200100000000000009000000000000001800000000000000F9FFFF6F000000000C000000000000000600000000000000C8010000000000000B000000000000001800000000000000050000000000000010020000000000000A0000000000000023000000000000000400000000000000F80100000000000000000000000000000000000000000000011101250E1305030E1017B44219110112060000022E001101120640186E0E030E3A0B3B0B49133F190000032400030E3E0B0B0B000000470000000400000000000801250000002C002300000000000000E8030100000000003C00000002E8030100000000003C000000016D0000000000000000010143000000031F0000000504006164645F64697370617463685F305F62726F6164636173745F365F66333200696E74002D004952454500310000000200000000004B000000260000006164645F64697370617463685F305F62726F6164636173745F365F6633320000000000160000000200000000004B00000043000000696E74000000000039000000040019000000010101FB0E0D000101010100000001000001002D0000000000000902E8030100000000000105010A82060B022C1202080001014952454500000000000000000000000000000000000000000000000000000000230000000002090008050200000000000000000000000000010000001200070024040100000000001400000000000000002E64796E73796D002E68617368002E64796E737472002E72656C612E64796E002E726F64617461002E65685F6672616D65002E74657874002E646174612E72656C2E726F002E64796E616D6963002E72656C726F5F70616464696E67002E64656275675F616262726576002E64656275675F696E666F002E64656275675F737472002E64656275675F7075626E616D6573002E64656275675F7075627479706573002E64656275675F6C696E65002E636F6D6D656E74002E73796D746162002E7368737472746162002E7374727461620000697265655F68616C5F65786563757461626C655F6C6962726172795F7175657279005F44594E414D494300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000B0000000200000000000000C801000000000000C801000000000000300000000000000003000000010000000800000000000000180000000000000009000000050000000200000000000000F801000000000000F80100000000000018000000000000000100000000000000040000000000000004000000000000000F000000030000000200000000000000100200000000000010020000000000002300000000000000000000000000000001000000000000000000000000000000170000000400000002000000000000003802000000000000380200000000000020010000000000000100000000000000080000000000000018000000000000002100000001000000020000000000000058030000000000005803000000000000480000000000000000000000000000000800000000000000000000000000000029000000010000000200000000000000A003000000000000A003000000000000480000000000000000000000000000000800000000000000000000000000000033000000010000000600000000000000E803010000000000E80300000000000050000000000000000000000000000000040000000000000000000000000000003900000001000000030000000000000040040200000000004004000000000000C8000000000000000000000000000000100000000000000000000000000000004600000006000000030000000000000008050200000000000805000000000000C0000000000000000300000000000000080000000000000010000000000000004F000000080000000300000000000000C805020000000000C805000000000000380A0000000000000000000000000000010000000000000000000000000000005E0000000100000000000000000000000000000000000000C80500000000000037000000000000000000000000000000010000000000000000000000000000006C0000000100000000000000000000000000000000000000FF050000000000004B000000000000000000000000000000010000000000000000000000000000007800000001000000300000000000000000000000000000004A060000000000002A0000000000000000000000000000000100000000000000010000000000000083000000010000000000000000000000000000000000000074060000000000003500000000000000000000000000000001000000000000000000000000000000930000000100000000000000000000000000000000000000A9060000000000001A00000000000000000000000000000001000000000000000000000000000000A30000000100000000000000000000000000000000000000C3060000000000003D00000000000000000000000000000001000000000000000000000000000000AF000000010000003000000000000000000000000000000000070000000000000500000000000000000000000000000001000000000000000100000000000000B8000000020000000000000000000000000000000000000008070000000000004800000000000000140000000200000008000000000000001800000000000000C000000003000000000000000000000000000000000000005007000000000000D200000000000000000000000000000001000000000000000000000000000000CA000000030000000000000000000000000000000000000022080000000000002C00000000000000000000000000000001000000000000000000000000000000"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ResolveRodataLoadsPass (iree-vm-resolve-rodata-loads) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.initializer {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  }
  vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.initializer {
  %null = vm.const.ref.zero : !vm.buffer
  %c2 = vm.const.i32 2
  %c7 = vm.const.i32 7
  %c1 = vm.const.i32 1
  %c14 = vm.const.i32 14
  %c-1 = vm.const.i64 -1
  %zero = vm.const.i64.zero
  %zero_0 = vm.const.i32.zero
  %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %nz = vm.cmp.nz.i64 %0#1 : i64
  %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
  %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  %2 = vm.select.i64 %1, %zero, %c-1 : i64
  %eq = vm.cmp.eq.i64 %2, %zero : i64
  vm.global.store.i32 %1, @_device_query_0 : i32
  vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.cond_br %eq, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
  %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.return
^bb2:  // pred: ^bb0
  vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c2 = vm.const.i32 2
  %c3 = vm.const.i32 3
  %c48 = vm.const.i32 48
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %c553648160 = vm.const.i32 553648160
  %c3_0 = vm.const.i64 3
  %c2_1 = vm.const.i64 2
  %c1_2 = vm.const.i64 1
  %zero_3 = vm.const.i64.zero
  %c24 = vm.const.i64 24
  %c4 = vm.const.i64 4
  %c-1 = vm.const.i64 -1
  %c-1_4 = vm.const.i32 -1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
  %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
  %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
  vm.cond_br %eq, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %1, ^bb4, ^bb3
^bb3:  // pred: ^bb2
  %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_12 : !vm.ref<!hal.buffer_view>
^bb4:  // pred: ^bb2
  vm.fail %1, "failed to wait on timepoint"
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.ex.file.from_memory(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %access : i32, %buffer : !vm.buffer, %offset : i64, %length : i64, %flags : i32) -> !vm.ref<!hal.file>
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.allocator.import(%allocator : !vm.ref<!hal.allocator>, %try : i32, %queue_affinity : i64, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {minimum_version = 1 : i32}
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.read(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_file : !vm.ref<!hal.file>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.write(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_file : !vm.ref<!hal.file>, %target_offset : i64, %length : i64, %flags : i32)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.devices.count() -> i32 attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb3
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  ^bb3:  // pred: ^bb1
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
#executable_target_embedded_elf_arm_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-arm_64", {cpu = "", cpu_features = "+reserve-x18", data_layout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32", native_vector_size = 16 : i64, target_triple = "aarch64-unknown-unknown-eabi-elf"}>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_arm_64_]>
module attributes {hal.device.targets = [#device_target_local], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
    vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c2 = vm.const.i32 2
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c3_0 = vm.const.i64 3
      %c2_1 = vm.const.i64 2
      %c1_2 = vm.const.i64 1
      %zero_3 = vm.const.i64.zero
      %c24 = vm.const.i64 24
      %c4 = vm.const.i64 4
      %c-1 = vm.const.i64 -1
      %c-1_4 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
      %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.br ^bb2
    ^bb2:  // 2 preds: ^bb0, ^bb1
      vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %1, ^bb4, ^bb3
    ^bb3:  // pred: ^bb2
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb4:  // pred: ^bb2
      vm.fail %1, "failed to wait on timepoint"
    }
    vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %c1 = vm.const.i32 1
      %c14 = vm.const.i32 14
      %c-1 = vm.const.i64 -1
      %zero = vm.const.i64.zero
      %zero_0 = vm.const.i32.zero
      %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %nz = vm.cmp.nz.i64 %0#1 : i64
      %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      %2 = vm.select.i64 %1, %zero, %c-1 : i64
      %eq = vm.cmp.eq.i64 %2, %zero : i64
      vm.global.store.i32 %1, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %eq, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    ^bb2:  // pred: ^bb0
      vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb3
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  ^bb3:  // pred: ^bb1
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb3
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  ^bb3:  // pred: ^bb1
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c2 = vm.const.i32 2
  %c3 = vm.const.i32 3
  %c48 = vm.const.i32 48
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %c553648160 = vm.const.i32 553648160
  %c3_0 = vm.const.i64 3
  %c2_1 = vm.const.i64 2
  %c1_2 = vm.const.i64 1
  %zero_3 = vm.const.i64.zero
  %c24 = vm.const.i64 24
  %c4 = vm.const.i64 4
  %c-1 = vm.const.i64 -1
  %c-1_4 = vm.const.i32 -1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
  %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
  %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
  vm.cond_br %eq, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.br ^bb2
^bb2:  // 2 preds: ^bb0, ^bb1
  vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %1, ^bb4, ^bb3
^bb3:  // pred: ^bb2
  %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_12 : !vm.ref<!hal.buffer_view>
^bb4:  // pred: ^bb2
  vm.fail %1, "failed to wait on timepoint"
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @__init() {
  %null = vm.const.ref.zero : !vm.buffer
  %c2 = vm.const.i32 2
  %c7 = vm.const.i32 7
  %c1 = vm.const.i32 1
  %c14 = vm.const.i32 14
  %c-1 = vm.const.i64 -1
  %zero = vm.const.i64.zero
  %zero_0 = vm.const.i32.zero
  %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %nz = vm.cmp.nz.i64 %0#1 : i64
  %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
  %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  %2 = vm.select.i64 %1, %zero, %c-1 : i64
  %eq = vm.cmp.eq.i64 %2, %zero : i64
  vm.global.store.i32 %1, @_device_query_0 : i32
  vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.cond_br %eq, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
  %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.return
^bb2:  // pred: ^bb0
  vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
}

// -----// IR Dump After Inliner (inline) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  }
}

// -----// IR Dump After CSE (cse) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf"} dense<"0x7F454C460201010000000000000000000300B700010000000000000000000000400000000000000050080000000000000000000040003800070040001500130006000000040000004000000000000000400000000000000040000000000000008801000000000000880100000000000008000000000000000100000004000000000000000000000000000000000000000000000000000000E803000000000000E80300000000000000000100000000000100000005000000E803000000000000E803010000000000E80301000000000050000000000000005000000000000000000001000000000001000000060000004004000000000000400402000000000040040200000000008801000000000000C00B00000000000000000100000000000200000006000000080500000000000008050200000000000805020000000000C000000000000000C000000000000000080000000000000052E57464040000004004000000000000400402000000000040040200000000008801000000000000C00B000000000000010000000000000051E574640600000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000120007002404010000000000140000000000000002000000020000000000000001000000000000000000000000697265655F68616C5F65786563757461626C655F6C6962726172795F717565727900000000000048040200000000000304000000000000580300000000000058040200000000000304000000000000E803010000000000600402000000000003040000000000006C03000000000000700402000000000003040000000000008B0300000000000088040200000000000304000000000000A00300000000000090040200000000000304000000000000A003000000000000A00402000000000003040000000000004004020000000000C00402000000000003040000000000005804020000000000C80402000000000003040000000000006803000000000000D00402000000000003040000000000006004020000000000E00402000000000003040000000000006804020000000000E804020000000000030400000000000080040200000000006164645F64697370617463685F300000000000006164645F64697370617463685F305F62726F6164636173745F365F663332006164645F746F73612E6D6C697200000000000000001000000000000000017A5200017C1E011B0C1F0018000000180000002C0001003C00000000480C1D109E029D0400000010000000340000004C000100140000000000000000000000FD7BBFA9FD030091281040F9E0031F2A0A2540A9080940F920C9404D4101C03D21D4204E0101803D410940FD20D4200E000900FDFD7BC1A8C0035FD61F1000711F2003D5A803081000019F9AC0035FD6000000000000000004000000000000000000000000000000000000000000000000000000000000000000000000000000010000000D00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001E000000000000000800000000000000FBFFFF6F000000000100000000000000070000000000000038020000000000000800000000000000200100000000000009000000000000001800000000000000F9FFFF6F000000000C000000000000000600000000000000C8010000000000000B000000000000001800000000000000050000000000000010020000000000000A0000000000000023000000000000000400000000000000F80100000000000000000000000000000000000000000000011101250E1305030E1017B44219110112060000022E001101120640186E0E030E3A0B3B0B49133F190000032400030E3E0B0B0B000000470000000400000000000801250000002C002300000000000000E8030100000000003C00000002E8030100000000003C000000016D0000000000000000010143000000031F0000000504006164645F64697370617463685F305F62726F6164636173745F365F66333200696E74002D004952454500310000000200000000004B000000260000006164645F64697370617463685F305F62726F6164636173745F365F6633320000000000160000000200000000004B00000043000000696E74000000000039000000040019000000010101FB0E0D000101010100000001000001002D0000000000000902E8030100000000000105010A82060B022C1202080001014952454500000000000000000000000000000000000000000000000000000000230000000002090008050200000000000000000000000000010000001200070024040100000000001400000000000000002E64796E73796D002E68617368002E64796E737472002E72656C612E64796E002E726F64617461002E65685F6672616D65002E74657874002E646174612E72656C2E726F002E64796E616D6963002E72656C726F5F70616464696E67002E64656275675F616262726576002E64656275675F696E666F002E64656275675F737472002E64656275675F7075626E616D6573002E64656275675F7075627479706573002E64656275675F6C696E65002E636F6D6D656E74002E73796D746162002E7368737472746162002E7374727461620000697265655F68616C5F65786563757461626C655F6C6962726172795F7175657279005F44594E414D494300000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000B0000000200000000000000C801000000000000C801000000000000300000000000000003000000010000000800000000000000180000000000000009000000050000000200000000000000F801000000000000F80100000000000018000000000000000100000000000000040000000000000004000000000000000F000000030000000200000000000000100200000000000010020000000000002300000000000000000000000000000001000000000000000000000000000000170000000400000002000000000000003802000000000000380200000000000020010000000000000100000000000000080000000000000018000000000000002100000001000000020000000000000058030000000000005803000000000000480000000000000000000000000000000800000000000000000000000000000029000000010000000200000000000000A003000000000000A003000000000000480000000000000000000000000000000800000000000000000000000000000033000000010000000600000000000000E803010000000000E80300000000000050000000000000000000000000000000040000000000000000000000000000003900000001000000030000000000000040040200000000004004000000000000C8000000000000000000000000000000100000000000000000000000000000004600000006000000030000000000000008050200000000000805000000000000C0000000000000000300000000000000080000000000000010000000000000004F000000080000000300000000000000C805020000000000C805000000000000380A0000000000000000000000000000010000000000000000000000000000005E0000000100000000000000000000000000000000000000C80500000000000037000000000000000000000000000000010000000000000000000000000000006C0000000100000000000000000000000000000000000000FF050000000000004B000000000000000000000000000000010000000000000000000000000000007800000001000000300000000000000000000000000000004A060000000000002A0000000000000000000000000000000100000000000000010000000000000083000000010000000000000000000000000000000000000074060000000000003500000000000000000000000000000001000000000000000000000000000000930000000100000000000000000000000000000000000000A9060000000000001A00000000000000000000000000000001000000000000000000000000000000A30000000100000000000000000000000000000000000000C3060000000000003D00000000000000000000000000000001000000000000000000000000000000AF000000010000003000000000000000000000000000000000070000000000000500000000000000000000000000000001000000000000000100000000000000B8000000020000000000000000000000000000000000000008070000000000004800000000000000140000000200000008000000000000001800000000000000C000000003000000000000000000000000000000000000005007000000000000D200000000000000000000000000000001000000000000000000000000000000CA000000030000000000000000000000000000000000000022080000000000002C00000000000000000000000000000001000000000000000000000000000000"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64} "embedded-elf-arm_64"
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::OrdinalAllocationPass (iree-vm-ordinal-allocation) //----- //
vm.module public @module attributes {ordinal_counts = #vm.ordinal_counts<import_funcs = 19, export_funcs = 2, internal_funcs = 2, global_bytes = 4, global_refs = 2, rodatas = 6, rwdatas = 0>} {
  vm.global.i32 private mutable @_device_query_0 {ordinal = 0 : i32} : i32
  vm.global.ref private mutable @_pipeline_layout_0 {ordinal = 0 : i32} : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_add_dispatch_0 {ordinal = 1 : i32} : !vm.ref<!hal.executable>
  vm.rodata private @add_dispatch_0_embedded_elf_arm_64 {alignment = 16 : i64, mime_type = "application/x-elf", ordinal = 0 : i32} dense<"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"> : vector<3472xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64, ordinal = 1 : i32} "hal.executable.format"
  vm.rodata private @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 {alignment = 1 : i64, ordinal = 2 : i32} "embedded-elf-arm_64"
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {ordinal = 0 : i32}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, ordinal = 1 : i32}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {ordinal = 2 : i32}
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, ordinal = 3 : i32}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {ordinal = 4 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {ordinal = 5 : i32}
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {ordinal = 6 : i32}
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {ordinal = 7 : i32}
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {ordinal = 8 : i32}
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, ordinal = 9 : i32}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, ordinal = 10 : i32}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, ordinal = 11 : i32}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {ordinal = 12 : i32}
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {ordinal = 13 : i32}
  vm.import private @hal.devices.get(%index : i32) -> !vm.ref<!hal.device> attributes {minimum_version = 2 : i32, nosideeffects, ordinal = 14 : i32}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, ordinal = 15 : i32}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {ordinal = 16 : i32}
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {ordinal = 17 : i32, vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, ordinal = 18 : i32}
  vm.rodata private @_utf8_input0_F74E046E5FFA3735 {alignment = 1 : i64, ordinal = 3 : i32} "input0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64, ordinal = 4 : i32} "tensor"
  vm.rodata private @_utf8_input1_E2E5222C371315B9 {alignment = 1 : i64, ordinal = 5 : i32} "input1"
  vm.func private @add(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}, ordinal = 0 : i32} {
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c2 = vm.const.i32 2
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c3_0 = vm.const.i64 3
    %c2_1 = vm.const.i64 2
    %c1_2 = vm.const.i64 1
    %zero_3 = vm.const.i64.zero
    %c24 = vm.const.i64 24
    %c4 = vm.const.i64 4
    %c-1 = vm.const.i64 -1
    %c-1_4 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_add_dispatch_0 = vm.global.load.ref @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_input0_F74E046E5FFA3735 = vm.const.ref.rodata @_utf8_input0_F74E046E5FFA3735 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input0_F74E046E5FFA3735, %c553648160, %c1, [%c3_0, %c2_1]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_5 = vm.call @hal.devices.get(%zero) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %ref_6 = vm.call @hal.device.allocator(%ref_5) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c24, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_input1_E2E5222C371315B9 = vm.const.ref.rodata @_utf8_input1_E2E5222C371315B9 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_input1_E2E5222C371315B9, %c553648160, %c1, [%c1_2, %c1_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC, %ref_6, %c4, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_8 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_9 = vm.call @hal.device.queue.alloca(%ref_5, %c-1, %null, %ref_8, %zero, %c48, %c3075, %c24) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_10 = vm.call @hal.command_buffer.create(%ref_5, %c1, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    %0 = vm.select.i64 %_device_query_0, %zero_3, %c-1 : i64
    %eq = vm.cmp.eq.i64 %0, %zero_3 : i64
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_3, %c24), (%c1, %zero, %ref_7, %zero_3, %c4), (%c2, %zero, %ref_9, %zero_3, %c24)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_add_dispatch_0, %zero, %c1, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.br ^bb2
  ^bb2:  // 2 preds: ^bb0, ^bb1
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_11 = vm.call @hal.fence.create(%ref_5, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_5, %c-1, %ref_8, %ref_11, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %1 = vm.call.variadic @hal.fence.await(%c-1_4, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %1, ^bb4, ^bb3
  ^bb3:  // pred: ^bb2
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_9, %zero_3, %c24, %c553648160, %c1, [%c3_0, %c2_1]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb4:  // pred: ^bb2
    vm.fail %1, "failed to wait on timepoint"
  }
  vm.export @add attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @add(%input0: tensor<3x2xf32>, %input1: tensor<1x1xf32>) -> (%output0: tensor<3x2xf32>)"}, ordinal = 0 : i32}
  vm.export @__init attributes {ordinal = 1 : i32}
  vm.func private @__init() attributes {ordinal = 1 : i32} {
    %null = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %c1 = vm.const.i32 1
    %c14 = vm.const.i32 14
    %c-1 = vm.const.i64 -1
    %zero = vm.const.i64.zero
    %zero_0 = vm.const.i32.zero
    %ref = vm.call @hal.devices.get(%zero_0) {nosideeffects} : (i32) -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 = vm.const.ref.rodata @_utf8_embedded_elf_arm_64_48330F48BB3EF6E0 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %nz = vm.cmp.nz.i64 %0#1 : i64
    %1 = vm.select.i32 %0#0, %nz, %zero_0 : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_0, [(%zero_0, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero_0)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_0, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    %2 = vm.select.i64 %1, %zero, %c-1 : i64
    %eq = vm.cmp.eq.i64 %2, %zero : i64
    vm.global.store.i32 %1, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %eq, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %add_dispatch_0_embedded_elf_arm_64 = vm.const.ref.rodata @add_dispatch_0_embedded_elf_arm_64 : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_embedded_elf_arm_64_48330F48BB3EF6E0, %add_dispatch_0_embedded_elf_arm_64, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.global.store.ref %ref_3, @_executable_add_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  ^bb2:  // pred: ^bb0
    vm.fail %c14, "none of the executable binaries in the module are supported by the runtime"
  }
}

