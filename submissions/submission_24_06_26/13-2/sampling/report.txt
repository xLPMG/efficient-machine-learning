####################################################################################################
# main.py output
####################################################################################################

# drop_last=False, batch_size = 3

(/work/EML/pytorch_env_new) [li83keq@login1 sampling]$ mpiexec -n 2 python main.py 
0 / 1 : DataLoader [tensor([120,  90, 110]), tensor([130,  20, 150]), tensor([40, 70])]
0 / 1 : BatchSampler [[12, 9, 11], [13, 2, 15], [4, 7]]
1 / 1 : DataLoader [tensor([100,  60,  80]), tensor([ 50, 140,   0]), tensor([30, 10])]
1 / 1 : BatchSampler [[10, 6, 8], [5, 14, 0], [3, 1]]

(/work/EML/pytorch_env_new) [li83keq@login1 sampling]$ mpiexec -n 4 python main.py 
1 / 3 : DataLoader [tensor([100,  80, 140]), tensor([30])]
2 / 3 : DataLoader [tensor([ 90, 130, 150]), tensor([70])]
0 / 3 : DataLoader [tensor([120, 110,  20]), tensor([40])]
0 / 3 : BatchSampler [[12, 11, 2], [4]]
2 / 3 : BatchSampler [[9, 13, 15], [7]]
1 / 3 : BatchSampler [[10, 8, 14], [3]]
3 / 3 : DataLoader [tensor([60, 50,  0]), tensor([10])]
3 / 3 : BatchSampler [[6, 5, 0], [1]]

# drop_last = True, batch_size = 3

(/work/EML/pytorch_env_new) [li83keq@login1 sampling]$ mpiexec -n 2 python main.py 
0 / 1 : DataLoader [tensor([120,  90, 110]), tensor([130,  20, 150])]
1 / 1 : DataLoader [tensor([100,  60,  80]), tensor([ 50, 140,   0])]
0 / 1 : BatchSampler [[12, 9, 11], [13, 2, 15]]
1 / 1 : BatchSampler [[10, 6, 8], [5, 14, 0]]

(/work/EML/pytorch_env_new) [li83keq@login1 sampling]$ mpiexec -n 4 python main.py 
1 / 3 : DataLoader [tensor([100,  80, 140])]
1 / 3 : BatchSampler [[10, 8, 14]]
3 / 3 : DataLoader [tensor([60, 50,  0])]
3 / 3 : BatchSampler [[6, 5, 0]]
0 / 3 : DataLoader [tensor([120, 110,  20])]
0 / 3 : BatchSampler [[12, 11, 2]]
2 / 3 : DataLoader [tensor([ 90, 130, 150])]
2 / 3 : BatchSampler [[9, 13, 15]]

####################################################################################################
Explanation
####################################################################################################

batch_size determines how many samples will be in one batch. If the total amount of samples is not
divisible by the batch size, we can see the effects of drop_last:
    - when set to False, the last batch will contain 1 to batch_size-1 samples
    - when set to True, the last batch will contain batch_size samples, because all remaining
    samples which are not enough to form another batch are simply dropped, e.g not used/included.

If the total amount of samples can be divided by the batch size, we can distribute the samples evenly 
among all batches and there is nothing to drop, thus the drop_last parameter won't change anything.